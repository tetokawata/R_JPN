# 平均差の推定 {#general}

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE)
```

## 概念整理

- 条件付き平均差 $\tau(x)=E[Y_i|D_i=d,X_i=x] - E[Y_i|D_i=d',X_i=x]$ の特徴を推定

  - ここでは $\tau = \tau(x)$ の平均値を推定

- 大きく２種類の推定方法を紹介

### Robinson推定

- 部分線形モデル [@robinson1988root] 上で $\tau$

$$E[Y|D=d,X=x]=\underbrace{\tau}_{Interest\ parameter}\times d+\underbrace{f(x)}_{Nuisance\ function}$$

- 線形モデルの一般化として解釈できる: $f(X)=\beta_0 + \beta_1X_1+...+\beta_LX_L$ と定式化すれば線形モデルと一致

- 推定手順: 
  
1. 部分線形モデルを変換

$$Y_i-\underbrace{E[Y_i|X_i]}_{Nuisance\ term}=\tau\times [D_i-\underbrace{E[D_i|X_i]}_{Nuisance\ term}]+u_i$$

2. $E[Y_i|X_i],E[D_i|X_i]$を予測関数として推定

3. 予測誤差間を単回帰

- 実際には$E[Y_i|X_i],E[D_i|X_i]$は未知の関数なので何らかの方法で推定する必要がある。関数の推定なので予測の手法が適用できる。

### Argument Inverse Propensity Score

- 2値の原因変数 $D_i=\{0,1\}$ を想定

- Double robust score $\phi$ を用いて推定

$$\phi_i(\tilde \mu_{1i},\tilde \mu_{0i},\tilde \mu_{Di})=\tilde \mu_{1i} - \tilde \mu_{0i} + \frac{D_i\times (Y_i-\tilde \mu_{1i})}{\tilde \mu_{Di}} + \frac{(1-D_i)\times (Y_i-\tilde \mu_{0i})}{1-\tilde \mu_{Di}}$$

- 以下が成立

$$\tau\equiv E[\tau(x)]=E[\phi_i(\mu_{1i},\mu_{0i},\mu_{Di})]$$

- ただし $\mu_{1i}=E[Y_i|D_i=1,X_i]$ , $\mu_{0i}=E[Y_i|D_i=0,X_i]$ , $\mu_{Di}=E[D_i|X_i]$

- 推定手順

1. $\mu_{1i},\mu_{0i},\mu_{Di}$ を予測関数として推定

2. 推定値 $\tilde \mu_{1i},\tilde \mu_{0i},\tilde \mu_{Di}$ を用いて、 $\phi_i(\tilde \mu_{1i},\tilde \mu_{0i},\tilde \mu_{Di})$ を計算

3. $\sum_i \phi_i(\tilde \mu_{1i},\tilde \mu_{0i},\tilde \mu_{Di})/N$ として $\tau$ を推定

## パッケージ

```{r}
library(tidyverse)
library(AER)
library(DoubleML)
library(mlr3)
library(mlr3learners)
library(data.table)
library(SuperLearner)
library(AIPW)
library(SuperLearner)
library(future.apply)
library(recipes)
library(estimatr)
```


## データ

```{r}
data("PSID1982")

set.seed(123)

Y <- PSID1982$wage |> log() # 結果変数

D <- if_else(PSID1982$occupation == "white",1,0)

X <- recipe(~ education + south + smsa + gender + ethnicity + industry + weeks,
            PSID1982) |> 
  step_other(all_nominal_predictors(),
             other = "others") |> 
  step_unknown(all_nominal_predictors()) |> 
  step_indicate_na(all_numeric_predictors()) |> 
  step_impute_median(all_numeric_predictors()) |> 
  step_dummy(all_nominal_predictors()) |> 
  step_zv(all_numeric_predictors()) |> 
  prep() |> 
  bake(PSID1982)

```

## Robinsons推定 (SuperLearner)

- 部分線形モデルをDouble Machine Learning法 [@chernozhukov2018double] で推定

- なんらかの方法（例、OLS、ランダムフォレスト、LASSO）で$E[Y|X],E[D|X]$の予測関数$f_Y(X),f_D(X)$を推定し、予測誤差を単回帰

- SuperLearner pakageを用いて推定

```{r}
fit.Y <- CV.SuperLearner(X = X,
                         Y = Y,
                         SL.library = c("SL.glmnet",
                                        "SL.lm",
                                        "SL.ranger")
                         )

fit.D <- CV.SuperLearner(X = X,
                         Y = D,
                         SL.library = c("SL.glmnet",
                                        "SL.lm",
                                        "SL.ranger")
                         )

Y.oht <- Y - fit.Y$SL.predict
D.oht <- D - fit.D$SL.predict

lm_robust(Y.oht ~ 0 + D.oht)
```


## Robinson推定 (DoubleML)

- DoubleMLパッケージ[@R-DoubleML]を利用

  - 機械学習についてのメタパッケージである、mlr3がベース

```{r}
learner <- 
  lrn("regr.ranger", 
      num.trees = 100) # Require bigger num.trees in practice

ml_g <- learner$clone()

ml_m <- learner$clone()

obj_dml_data <- 
  double_ml_data_from_matrix(X = X,
                             y = as.numeric(Y),
                             d = as.numeric(D))

dml_plr_obj <- 
  DoubleMLPLR$new(obj_dml_data, 
                  ml_g, 
                  ml_m, 
                  dml_procedure="dml1", 
                  n_rep = 3)

dml_plr_obj$fit()
```

```{r}
print(dml_plr_obj)
```

## AIPW (AIPW)

- AIPWパッケージを用いて、AIPW推定

  - SuperLearnerパッケージがベース

```{r}
plan(multisession, workers = 8, gc = T)
algorism <- AIPW$new(
  Y = Y,
  A = D,
  W = X,
  Q.SL.library = c(
    "SL.lm",
    "SL.ranger",
    "SL.glmnet"
  ),
  g.SL.library = c(
    "SL.lm",
    "SL.ranger",
    "SL.glmnet"
  )
)

algorism$stratified_fit()$summary()
```

### Balance check

- 0あるいは1に非常に近い $\tilde\mu_{Di}$ が存在する場合、AIPW推定は極めて不安定

  - Default設定では, $\tilde \mu_{Di} \le 0.025$ または $\tilde \mu_{Di} \ge 0.975$ であればサンプル $i$ は推定から排除される

- 平均差の推定の前に、 $\tilde\mu_{Di}$ の分布を確認する必要がある

- $\tilde\mu_{Di}$ の分布

```{r}
algorism$plot.p_score()
```
