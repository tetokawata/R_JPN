
# Constant parameterの推定 {#unique}

- （条件付き）平均差を推定する。

- 点推定だけでなく、信頼区間も推定する。

## データ

```{r}
library(tidyverse)

data("NMES1988",
     package = "AER")

raw <- NMES1988
```

## 部分線形モデルに基づく推定

- 部分線形モデルに関心のあるパラメータを埋め込む

$$E[Y|D=d,X=x]=\underbrace{\tau}_{Interest\ parameter}\times d+\underbrace{f(x)}_{Nuisance\ function}$$

### OLS by lm_robust (estimatr)


- $\tau(x)=\tau,f(x)=\beta_0+\beta_1x_1+...+\beta_Lx_L$と特定化

- サンプル内MSEを最大化するように推定

- robust standard errorを計算するためにestimatrパッケージ[@R-estimatr]を利用

```{r}
library(estimatr)
```

- lm_robust関数で推定

```{r}
lm_robust(visits ~ insurance + age + gender + school + income + employed + region + afam + married,
          data = raw)
```

#### 発展:推計結果表{-}

- tidy関数により推定結果data.frameに変化することで、kable関数(knitrパッケージ)による推計結果表の整形、geom_pointrange関数による可視化が可能

- 点推定値(estimate)、標準誤差(std.error)のみを残した推計結果表

```{r}
library(knitr)
library(tidyverse)

fit <- 
  lm_robust(visits ~ insurance + age + gender + school + income + employed + region + afam + married,
            data = raw)

fit <- tidy(fit)

fit <- select(fit, term, estimate, std.error)

kable(fit, digits = 2)
```

```{r}
fit <- filter(fit,
              term == "insuranceyes")
kable(fit, digits = 2)
```


#### 発展:Dot-and-Whisker plotによる可視化{-}

- Dot-and-Whisker図により点推定量と信頼区間を可視化

```{r}
fit <- 
  lm_robust(visits ~ insurance + age + gender + school + income + employed + region + afam + married,
            data = raw)

fit <- tidy(fit)

fit <- filter(fit,
              term != "(Intercept)")

ggplot(fit, aes(y = term,
                x = estimate,
                xmin = conf.low,
                xmax = conf.high)) +
  geom_pointrange()
```


```{r}
fit <- filter(fit,
              term == "insuranceyes")

ggplot(fit, aes(y = term,
                x = estimate,
                xmin = conf.low,
                xmax = conf.high)) +
  geom_pointrange() +
  geom_vline(xintercept = 0)
```

### ロビンソン変換

- 部分線形モデルをロビンソン変換[@robinson1988root]

$$Y_i-\underbrace{E[Y_i|X_i]}_{Nuisance\ term}=\tau\times [D_i-\underbrace{E[D_i|X_i]}_{Nuisance\ term}]+u_i$$

- $E[Y_i|X_i],E[D_i|X_i]$を予測関数として推定し、予測誤差間を単回帰すればよい

- 実際には$E[Y_i|X_i],E[D_i|X_i]$は未知の関数なので何らかの方法で推定する必要がある。関数の推定なので予測の手法が適用できる。

#### Double selection: rlassoEffect (hdm)

- 2重選択法[@belloni2014inference]を紹介

- LASSOにより$Y_i,D_i$の両方あるいはどちらか一方を予測する上でrelevantな$X^c$を特定しコントロールする

  - $Y_i,D_i$どちらの予測にもrelevantではない変数は除外する

- hdmパッケージ[@R-hdm]を利用

```{r}
library(hdm)

Y <- raw$visits

D <- if_else(raw$insurance == "yes", 1, 0)

X <- model.matrix(~ - 1+ age + gender + school + income + employed + region + afam + married,
                  raw)

fit <-
  rlassoEffect(x = X,
               y = Y,
               d = D,
               method = "double selection")
```

- 推定結果

```{r}
summary(fit)
```

- 選択されたコントロール変数

```{r}
fit$selection.index
```


#### Double Machine Learning (DoubleML)

- Double Machine Learning法[@chernozhukov2018double]を紹介

- なんらかの方法（例、OLS、ランダムフォレスト、LASSO）で$E[Y|X],E[D|X]$の予測関数$f_Y(X),f_D(X)$を推定し、予測誤差を単回帰

- DoubleMLパッケージ[@R-DoubleML]を利用

```{r}
library(DoubleML)
library(mlr3)
library(mlr3learners)
library(data.table)

learner <- 
  lrn("regr.ranger", 
      num.trees = 100) # Require bigger num.trees in practice

ml_g <- learner$clone()

ml_m <- learner$clone()

obj_dml_data <- 
  double_ml_data_from_matrix(X = X,
                             y = Y,
                             d = D)

dml_plr_obj <- 
  DoubleMLPLR$new(obj_dml_data, 
                  ml_g, 
                  ml_m, 
                  dml_procedure="dml1", 
                  n_rep = 3)

dml_plr_obj$fit()
```

```{r}
print(dml_plr_obj)
```
