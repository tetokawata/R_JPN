[["index.html", "Rによる比較・予測・因果推論入門 ver0.1 はじめに", " Rによる比較・予測・因果推論入門 ver0.1 川田恵介 2021-05-05 はじめに 本ページでは、定量的な比較、（反実仮想）因果推定、予測分析をRによって行う方法を紹介します。 データインポート、整理、可視化を行う関数群を統合的に提供するtidyverseパッケージ(Wickham et al. 2019)の利用を前提にします。 AERパッケージ(Kleiber and Zeileis 2008)に含まれるNMES1988を例として使用します。 無料で公開されている有力な参考文献として、以下を推薦します。 機械学習を用いた予測：James et al. (2013) 公開ページ 日本語によるR入門：私たちのR: ベストプラクティスの探究 tidyverseパッケージの利用： 公式ページ "],["intro.html", "Chapter 1 準備", " Chapter 1 準備 オフライン環境の整備 Rのインストール R studioのインストール オンライン環境の整備 R cloudへの登録 "],["目標.html", "Chapter 2 目標 2.1 予測 2.2 比較 2.3 因果効果", " Chapter 2 目標 経済学におけるデータ分析の大部分は、複数の変数間での関係性の理解・利用を目的とします。 ここではある結果変数\\(Y\\)と独立変数（群）\\(X=X_1,...,X_L\\)の関係性に焦点を当てます。 より具体的な目標は大きく（予測）\\(Y\\)の予測関数の推定、（比較）異なる\\(X\\)間での\\(Y\\)の比較、（因果効果）\\(X\\)の変化が\\(Y\\)に与える因果効果の推定、に大別できます。 \\(Y\\)と\\(X\\)がともに観察でき、関心のある母集団からランダムサンプリングされたデータが入手出来ているとします。 2.1 予測 Chapter 5 の背後にある問題意識・方針を紹介します。 2.1.1 問題設定 データと同じ母集団から新しくランダムサンプリングされ、\\(X\\)のみが観察できるサンプルについて\\(Y\\)の値を予測することを目的とします。 具体的には、事前に定義する損失関数の期待値を最小化するような、予測関数\\(f(X)\\)の推定をめざします。 以下ではMean squared error(MSE)を損失関数として用います。 所与の\\(f(X)\\)、母分布に従う確率変数\\(Y,X\\)についてMSEは以下のように定義されます。 \\[MSE = E_{X,Y}[(Y_i-f(X_i))^2]\\] 一般にMSEは以下のように書き換えられます。 \\[MSE = \\underbrace{E_{X,Y}[(Y_i-\\bar{Y}(X_i))^2]}_{Irreducible\\ error}+\\underbrace{E_{X,Y}[(\\bar{Y}(X_i)-f(X_i))^2]}_{Reducible\\ error}\\] ただし\\(\\bar{Y}(X_i)=E[Y_i|X_i]\\)。 上記式から以下が確認できます。 最善の予測関数は条件付き母平均\\(\\bar{Y}(X_i)\\)(Reducible error = 0) 最善の予測関数のもとでも削減不可能なエラー(Irreducible error)が存在 予測関数の推定 \\(=\\) Reducible errorの削減 \\(=\\) 条件付き母平均との乖離(MSE)の削減 2.1.2 Bias-Variance tradeoff 実際の\\(f(X_i)\\)はランダムサンプリングされたデータから推定される必要があり、実際には確率分布を持ちます。 Reducible errorは一般に以下のように書き換えられます。 \\[E_{Y,X,f(X)}[(\\bar{Y}(X_i)-f(X_i))^2]\\] \\[=\\underbrace{(E_{Y,X,f(X)}[\\bar{Y}(X_i)-\\bar{f}(X_i)])^2}_{Bias}+\\underbrace{E_{Y,X,f(X)}[(\\bar{f}(X_i)-f(X_i))^2]}_{Variance}.\\] ただし\\(\\bar{f}(X_i)=f(X_i)\\)。 上記式は推定される予測関数が平均的にどの程度条件付き母平均を近似できているのか(Biasがどの程度小さいのか)だけでなく、予測関数の分布がどの程度散らばっているのか（VArianceがどの程度大木のか）、についても考慮する必要があることを示しています。 母平均\\(\\bar Y(X_i)\\)が単純な既知の関数形に従い、かつサンプルサイズが大きい場合、OLS推定された\\(f(X_i)\\)は\\(Bias=0\\)かつ小さいVarianceを達成します。 しかしながら社会科学における多くの応用においては、\\(\\bar Y(X_i)\\)は未知かつ複雑であることが予想され、その複雑さに対してサンプルサイズが小さいことを想定する必要があります。 このような状況では、OLSやサブサンプル平均により推定された予測モデルは、Bias-Variance tradeoffに直面します。 少ないパラメータ（短い回帰式、少ないサブサンプル分割）を推定する場合、大きなBiasを持つ 多くのパラメータ（長い回帰式、多いサブサンプル分割）を推定する場合、大きなVarianceを持つ。 Bias-variance tradeoffを分析者が解くこと（最善のモデル設定を行うこと）は困難です。Chapter 5 で紹介するLASSO/Ridge/Random Forestなどの手法は、bias-varianceのバランスをよりデータ主導型かつ現実的な計算時間で達成することを目指します。 2.2 比較 Chapter 6 の背後にある問題意識・方針を紹介します。 2.2.1 問題設定 変数\\(D\\)の値が異なる集団間において、結果変数\\(Y\\)の分布がどの程度異なっているのか、推定します。 その際直接的な関心ではない変数群\\(X\\)は一定であるとします。 以下の議論では\\(Y\\)の平均値に焦点を当て、\\(E[Y|D=d,X]-E[Y|D=d&#39;,X]\\)の推定を目指します。 またこの際に点推定量のみならず、信頼区間の推定も行います。 2.2.2 High-dimentional X 多数の\\(X\\)で条件づける必要がある場合、重回帰やマッチング法などの手法の有効性が失われます。 これは予測で問題となった大きすぎるvarianceが生じてしまうためです。 なお\\(X\\)が少数であったとしても、定式化の自由度（高次項や交差項の導入）を持たせた場合、同様の問題が生じます。 この問題を回避するためにChapter 6 では、LASSOやRandom Forestなどの予測手法の応用を紹介します。 2.3 因果効果 引き続きChapter 6 の背後にある問題意識・方針を紹介します。 2.3.1 問題設定 ある集団の変数\\(D\\)を変化させた場合、結果変数\\(Y\\)の分布がどのように変化するのか、因果効果を推定します。 2.3.2 識別の問題 因果効果を推定する際には、識別条件をまず議論する必要があります。 識別条件：「仮にサンプルサイズが無限大である」場合、どのような仮定の下で因果効果を推定できるか？ 本ページでは\\(D\\)の条件付きランダム化（\\(X\\)が均一のグループ内では、\\(D\\)の値がランダムに決定されている）の仮定に基づき議論していきます。 この仮定のもとで\\(D\\)の因果効果は、\\(E[Y|D=d,X]-E[Y|D=d&#39;,X]\\)によって識別できます。 代替的な識別条件も複数存在 2.3.3 推定の問題 識別条件はしばしば多数の\\(X\\)について、条件づけた平均差の推定を要求されます このような状況では比較の問題と同様に、機械学習の応用が有益となります。 "],["データ整備.html", "Chapter 3 データ整備 3.1 新しい変数の作成: mutate (tidyverse) 3.2 変数の限定: select (tidyverse) 3.3 サンプルの除外:filter (tidyverse)", " Chapter 3 データ整備 AERパッケージに含まれるNMES1988を利用 data(&quot;NMES1988&quot;, package = &quot;AER&quot;) raw &lt;- NMES1988 tidyverseパッケージに含まれるdplyrパッケージ(Wickham et al. 2021)は、有用な関数を提供 library(tidyverse) 3.1 新しい変数の作成: mutate (tidyverse) mutate関数の利用 df &lt;- mutate(raw, square_age = age^2, log_age = log(age), total_visit = visits + nvisits + ovisits + novisits ) 3.2 変数の限定: select (tidyverse) select関数の利用 df &lt;- select(raw, age, visits ) 特定の変数の除外 df &lt;- select(raw, -age, -visits ) 3.3 サンプルの除外:filter (tidyverse) filter関数の利用 df &lt;- filter(raw, visits &gt;= 5 ) "],["可視化.html", "Chapter 4 可視化 4.1 Y=連続、Xカテゴリ 4.2 Y=連続、X=連続", " Chapter 4 可視化 tidyverseに含まれるggplot2パッケージ(Wickham et al. 2020)を利用 library(tidyverse) 4.1 Y=連続、Xカテゴリ 4.1.1 ヒストグラム: geom_histogram (tidyverse) 医療機関の利用回数 ggplot(raw, aes(x = visits) ) + geom_histogram() メディケイド保有別の利用回数 ggplot(raw, aes(x = visits, fill = medicaid) ) + geom_histogram(position = &quot;identity&quot;, alpha = 0.5) 男女・メディケイド保有別の利用回数 ggplot(raw, aes(x = visits, fill = medicaid) ) + geom_histogram(position = &quot;identity&quot;, alpha = 0.5 ) + facet_wrap(~ gender) 4.1.2 密度: geom_density (tidyverse) 男女・メディケイド保有別の利用回数 ggplot(raw, aes(x = visits, fill = medicaid) ) + geom_density(position = &quot;identity&quot;, alpha = 0.5 ) + facet_wrap(~ gender) 4.1.3 Boxplot: geom_boxplot (tidyverse) ggplot(raw, aes(y = visits, x = medicaid) ) + geom_boxplot() 4.2 Y=連続、X=連続 4.2.1 散布図: geom_point (tidyverse) 散布図：連続変数間の関係性を可視化する図 ggplot(raw, aes(x = income, y = visits) ) + geom_point() サンプルサイズが大きくなると機能しない 4.2.2 ヒートマップ: geom_bin2d (tidyverse) 代替案はヒートマップ ggplot(raw, aes(x = income, y = visits) ) + geom_bin2d() "],["prediction.html", "Chapter 5 予測関数の推定 5.1 データの導入 5.2 データ分割 5.3 OLS 5.4 LASSO 5.5 Rdige 5.6 Bagging 5.7 Random Forest", " Chapter 5 予測関数の推定 5.1 データの導入 data(&quot;NMES1988&quot;, package = &quot;AER&quot;) raw &lt;- NMES1988 5.2 データ分割 ここでは5個のデータに分割する。 group &lt;- sample(1:5, size = nrow(raw), replace = TRUE) 5.3 OLS 線形予測関数\\(f(X)=\\beta_0 + \\beta_1X_1+...+\\beta_LX_L\\)を仮定し、最小二乗法にて推定する。 fit &lt;- lm(visits ~ ., data = raw[group != 1,]) coef(fit) ## (Intercept) nvisits ovisits novisits emergency ## 2.11599183 0.26612435 0.07526872 0.11240549 0.13035369 ## hospital healthpoor healthexcellent chronic adllimited ## 1.37533622 2.06403981 -1.22918752 0.88331150 0.10926354 ## regionnortheast regionmidwest regionwest age afamyes ## 0.60479378 -0.37761662 0.15450166 -0.13075635 -0.38941124 ## gendermale marriedyes school income employedyes ## -0.46133687 -0.17833078 0.12743970 -0.02971627 0.52929491 ## insuranceyes medicaidyes ## 1.37693280 1.46379586 予測値の導出 Y.hat &lt;- predict(fit,raw) テストデータへの適合 mean((raw$visits - Y.hat)[group == 1]^2) ## [1] 35.08665 訓練データへの適合 mean((raw$visits - Y.hat)[group != 1]^2) ## [1] 38.52352 5.4 LASSO glmentパッケージ(Friedman et al. 2021)を利用 library(glmnet) Y &lt;- raw$visits X &lt;- model.matrix(visits ~ -1 + ., data = raw) cv &lt;- cv.glmnet(x = X[group != 1,], y = Y[group != 1], alpha = 1) fit &lt;- glmnet(x = X[group != 1,], y = Y[group != 1], alpha = 1, lambda = cv$lambda.min) 予測モデルの確認 coef(fit) ## 23 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## s0 ## (Intercept) 1.72647011 ## nvisits 0.26148829 ## ovisits 0.06630306 ## novisits 0.10557520 ## emergency 0.10838457 ## hospital 1.36008477 ## healthpoor 1.99053922 ## healthaverage . ## healthexcellent -1.10240377 ## chronic 0.87547893 ## adllimited 0.03391850 ## regionnortheast 0.50268862 ## regionmidwest -0.34073914 ## regionwest 0.05844919 ## age -0.05051568 ## afamyes -0.26262962 ## gendermale -0.41943384 ## marriedyes -0.10787744 ## school 0.11844568 ## income -0.01346280 ## employedyes 0.36940235 ## insuranceyes 1.24031029 ## medicaidyes 1.26632690 予測値の導出 Y.hat &lt;- predict(fit,X) テストデータへの適合 mean((Y - Y.hat)[group == 1]^2) ## [1] 34.92133 訓練データへの適合 mean((Y - Y.hat)[group != 1]^2) ## [1] 38.54843 5.5 Rdige 引き続きglmentパッケージ(Friedman et al. 2021)を利用 cv &lt;- cv.glmnet(x = X[group != 1,], y = Y[group != 1], alpha = 0) fit &lt;- glmnet(x = X[group != 1,], y = Y[group != 1], alpha = 0, lambda = cv$lambda.min) 予測モデルの確認 coef(fit) ## 23 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## s0 ## (Intercept) 2.91334337 ## nvisits 0.23823548 ## ovisits 0.07468668 ## novisits 0.10632749 ## emergency 0.22721378 ## hospital 1.22417925 ## healthpoor 1.56645161 ## healthaverage -0.38038254 ## healthexcellent -1.49408146 ## chronic 0.79920484 ## adllimited 0.20869689 ## regionnortheast 0.54583525 ## regionmidwest -0.34259820 ## regionwest 0.16462404 ## age -0.12216590 ## afamyes -0.39538494 ## gendermale -0.41800894 ## marriedyes -0.15765222 ## school 0.11379541 ## income -0.02450203 ## employedyes 0.42123793 ## insuranceyes 1.17419208 ## medicaidyes 1.15915382 予測値の導出 Y.hat &lt;- predict(fit,X) テストデータへの適合 mean((Y - Y.hat)[group == 1]^2) ## [1] 34.83197 訓練データへの適合 mean((Y - Y.hat)[group != 1]^2) ## [1] 38.594 5.6 Bagging rangerパッケージ(Wright, Wager, and Probst 2020)を利用 library(ranger) fit &lt;- ranger(x = X[group != 1,], y = Y[group != 1], num.trees = 2000, mtry = ncol(X)) 予測値の計算 Y.hat &lt;- predict(fit,X)$predictions テストデータへの適合 mean((Y - Y.hat)[group == 1]^2) ## [1] 35.38 訓練データへの適合 mean((Y - Y.hat)[group != 1]^2) ## [1] 9.016629 5.7 Random Forest rangerパッケージ(Wright, Wager, and Probst 2020)を利用 library(ranger) fit &lt;- ranger(x = X[group != 1,], y = Y[group != 1], num.trees = 2000) 予測値の計算 Y.hat &lt;- predict(fit,X)$predictions テストデータへの適合 mean((Y - Y.hat)[group == 1]^2) ## [1] 32.81325 訓練データへの適合 mean((Y - Y.hat)[group != 1]^2) ## [1] 12.8627 "],["unique.html", "Chapter 6 Constant parameterの推定 6.1 データ 6.2 部分線形モデルに基づく推定", " Chapter 6 Constant parameterの推定 （条件付き）平均差を推定する。 点推定だけでなく、信頼区間も推定する。 6.1 データ library(tidyverse) data(&quot;NMES1988&quot;, package = &quot;AER&quot;) raw &lt;- NMES1988 6.2 部分線形モデルに基づく推定 部分線形モデルに関心のあるパラメータを埋め込む \\[E[Y|D=d,X=x]=\\underbrace{\\tau}_{Interest\\ parameter}\\times d+\\underbrace{f(x)}_{Nuisance\\ function}\\] 6.2.1 OLS by lm_robust (estimatr) \\(\\tau(x)=\\tau,f(x)=\\beta_0+\\beta_1x_1+...+\\beta_Lx_L\\)と特定化 サンプル内MSEを最大化するように推定 robust standard errorを計算するためにestimatrパッケージ(Blair et al. 2021)を利用 library(estimatr) lm_robust関数で推定 lm_robust(visits ~ insurance + age + gender + school + income + employed + region + afam + married, data = raw) ## Estimate Std. Error t value Pr(&gt;|t|) CI Lower ## (Intercept) 4.32086605 1.30928537 3.3001713 9.739642e-04 1.75400683 ## insuranceyes 0.96591490 0.24785140 3.8971533 9.877947e-05 0.48000123 ## age 0.04293421 0.15977094 0.2687235 7.881551e-01 -0.27029737 ## gendermale -0.46726544 0.22050124 -2.1191057 3.413748e-02 -0.89955902 ## school 0.08475793 0.03108035 2.7270588 6.415540e-03 0.02382479 ## income -0.04678801 0.03712934 -1.2601357 2.076873e-01 -0.11958023 ## employedyes -0.34186375 0.42407955 -0.8061312 4.202108e-01 -1.17327342 ## regionnortheast 0.34814737 0.30392728 1.1454957 2.520663e-01 -0.24770327 ## regionmidwest -0.40583622 0.25526934 -1.5898354 1.119439e-01 -0.90629278 ## regionwest 0.57163030 0.30418463 1.8792215 6.028038e-02 -0.02472489 ## afamyes -0.39294341 0.34701206 -1.1323624 2.575439e-01 -1.07326195 ## marriedyes -0.29559842 0.23244624 -1.2716851 2.035523e-01 -0.75131020 ## CI Upper DF ## (Intercept) 6.88772528 4394 ## insuranceyes 1.45182856 4394 ## age 0.35616578 4394 ## gendermale -0.03497187 4394 ## school 0.14569108 4394 ## income 0.02600421 4394 ## employedyes 0.48954591 4394 ## regionnortheast 0.94399802 4394 ## regionmidwest 0.09462034 4394 ## regionwest 1.16798549 4394 ## afamyes 0.28737512 4394 ## marriedyes 0.16011336 4394 発展:推計結果表 tidy関数により推定結果data.frameに変化することで、kable関数(knitrパッケージ)による推計結果表の整形、geom_pointrange関数による可視化が可能 点推定値(estimate)、標準誤差(std.error)のみを残した推計結果表 library(knitr) library(tidyverse) fit &lt;- lm_robust(visits ~ insurance + age + gender + school + income + employed + region + afam + married, data = raw) fit &lt;- tidy(fit) fit &lt;- select(fit, term, estimate, std.error) kable(fit, digits = 2) term estimate std.error (Intercept) 4.32 1.31 insuranceyes 0.97 0.25 age 0.04 0.16 gendermale -0.47 0.22 school 0.08 0.03 income -0.05 0.04 employedyes -0.34 0.42 regionnortheast 0.35 0.30 regionmidwest -0.41 0.26 regionwest 0.57 0.30 afamyes -0.39 0.35 marriedyes -0.30 0.23 fit &lt;- filter(fit, term == &quot;insuranceyes&quot;) kable(fit, digits = 2) term estimate std.error insuranceyes 0.97 0.25 発展:Dot-and-Whisker plotによる可視化 Dot-and-Whisker図により点推定量と信頼区間を可視化 fit &lt;- lm_robust(visits ~ insurance + age + gender + school + income + employed + region + afam + married, data = raw) fit &lt;- tidy(fit) fit &lt;- filter(fit, term != &quot;(Intercept)&quot;) ggplot(fit, aes(y = term, x = estimate, xmin = conf.low, xmax = conf.high)) + geom_pointrange() fit &lt;- filter(fit, term == &quot;insuranceyes&quot;) ggplot(fit, aes(y = term, x = estimate, xmin = conf.low, xmax = conf.high)) + geom_pointrange() + geom_vline(xintercept = 0) 6.2.2 ロビンソン変換 部分線形モデルをロビンソン変換(Robinson 1988) \\[Y_i-\\underbrace{E[Y_i|X_i]}_{Nuisance\\ term}=\\tau\\times [D_i-\\underbrace{E[D_i|X_i]}_{Nuisance\\ term}]+u_i\\] \\(E[Y_i|X_i],E[D_i|X_i]\\)を予測関数として推定し、予測誤差間を単回帰すればよい 実際には\\(E[Y_i|X_i],E[D_i|X_i]\\)は未知の関数なので何らかの方法で推定する必要がある。関数の推定なので予測の手法が適用できる。 6.2.2.1 Double selection: rlassoEffect (hdm) 2重選択法(Belloni, Chernozhukov, and Hansen 2014)を紹介 LASSOにより\\(Y_i,D_i\\)の両方あるいはどちらか一方を予測する上でrelevantな\\(X^c\\)を特定しコントロールする \\(Y_i,D_i\\)どちらの予測にもrelevantではない変数は除外する hdmパッケージ(Spindler, Chernozhukov, and Hansen 2019)を利用 library(hdm) Y &lt;- raw$visits D &lt;- if_else(raw$insurance == &quot;yes&quot;, 1, 0) X &lt;- model.matrix(~ - 1+ age + gender + school + income + employed + region + afam + married, raw) fit &lt;- rlassoEffect(x = X, y = Y, d = D, method = &quot;double selection&quot;) 推定結果 summary(fit) ## [1] &quot;Estimates and significance testing of the effect of target variables&quot; ## Estimate. Std. Error t value Pr(&gt;|t|) ## d1 0.9597 0.2480 3.87 0.000109 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 選択されたコントロール変数 fit$selection.index ## age genderfemale gendermale school income ## FALSE FALSE FALSE TRUE TRUE ## employedyes regionnortheast regionmidwest regionwest afamyes ## FALSE FALSE TRUE FALSE TRUE ## marriedyes ## TRUE 6.2.2.2 Double Machine Learning (DoubleML) Double Machine Learning法(Chernozhukov et al. 2018)を紹介 なんらかの方法（例、OLS、ランダムフォレスト、LASSO）で\\(E[Y|X],E[D|X]\\)の予測関数\\(f_Y(X),f_D(X)\\)を推定し、予測誤差を単回帰 DoubleMLパッケージ(Bach et al. 2021)を利用 library(DoubleML) library(mlr3) library(mlr3learners) library(data.table) learner &lt;- lrn(&quot;regr.ranger&quot;, num.trees = 100) # Require bigger num.trees in practice ml_g &lt;- learner$clone() ml_m &lt;- learner$clone() obj_dml_data &lt;- double_ml_data_from_matrix(X = X, y = Y, d = D) dml_plr_obj &lt;- DoubleMLPLR$new(obj_dml_data, ml_g, ml_m, dml_procedure=&quot;dml1&quot;, n_rep = 3) dml_plr_obj$fit() ## INFO [04:55:45.630] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_g&#39; (iter 3/5) ## INFO [04:55:45.981] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_g&#39; (iter 4/5) ## INFO [04:55:46.280] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_g&#39; (iter 5/5) ## INFO [04:55:46.527] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_g&#39; (iter 1/5) ## INFO [04:55:46.749] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_g&#39; (iter 2/5) ## INFO [04:55:47.148] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_m&#39; (iter 3/5) ## INFO [04:55:47.333] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_m&#39; (iter 1/5) ## INFO [04:55:47.523] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_m&#39; (iter 2/5) ## INFO [04:55:47.706] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_m&#39; (iter 4/5) ## INFO [04:55:47.889] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_m&#39; (iter 5/5) ## INFO [04:55:48.158] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_g&#39; (iter 2/5) ## INFO [04:55:48.366] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_g&#39; (iter 1/5) ## INFO [04:55:48.582] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_g&#39; (iter 3/5) ## INFO [04:55:48.806] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_g&#39; (iter 4/5) ## INFO [04:55:49.074] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_g&#39; (iter 5/5) ## INFO [04:55:49.406] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_m&#39; (iter 2/5) ## INFO [04:55:49.616] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_m&#39; (iter 1/5) ## INFO [04:55:49.814] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_m&#39; (iter 3/5) ## INFO [04:55:50.003] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_m&#39; (iter 4/5) ## INFO [04:55:50.203] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_m&#39; (iter 5/5) ## INFO [04:55:50.464] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_g&#39; (iter 3/5) ## INFO [04:55:50.677] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_g&#39; (iter 5/5) ## INFO [04:55:50.889] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_g&#39; (iter 1/5) ## INFO [04:55:51.112] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_g&#39; (iter 4/5) ## INFO [04:55:51.322] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_g&#39; (iter 2/5) ## INFO [04:55:51.615] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_m&#39; (iter 5/5) ## INFO [04:55:51.799] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_m&#39; (iter 3/5) ## INFO [04:55:51.983] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_m&#39; (iter 1/5) ## INFO [04:55:52.172] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_m&#39; (iter 4/5) ## INFO [04:55:52.355] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_m&#39; (iter 2/5) print(dml_plr_obj) ## ================= DoubleMLPLR Object ================== ## ## ## ------------------ Data summary ------------------ ## Outcome variable: y ## Treatment variable(s): d ## Covariates: X1, X2, X3, X4, X5, X6, X7, X8, X9, X10, X11 ## Instrument(s): ## No. Observations: 4406 ## ## ------------------ Score &amp; algorithm ------------------ ## Score function: partialling out ## DML algorithm: dml1 ## ## ------------------ Machine learner ------------------ ## ml_g: regr.ranger ## ml_m: regr.ranger ## ## ------------------ Resampling ------------------ ## No. folds: 5 ## No. repeated sample splits: 3 ## Apply cross-fitting: TRUE ## ## ------------------ Fit summary ------------------ ## [1] &quot;Estimates and significance testing of the effect of target variables&quot; ## Estimate. Std. Error t value Pr(&gt;|t|) ## d 1.2215 0.2638 4.63 3.66e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 "],["references.html", "References", " References Bach, Philipp, Victor Chernozhukov, Malte S. Kurz, and Martin Spindler. 2021. DoubleML: Double Machine Learning in r. https://CRAN.R-project.org/package=DoubleML. Belloni, Alexandre, Victor Chernozhukov, and Christian Hansen. 2014. “Inference on Treatment Effects After Selection Among High-Dimensional Controls.” The Review of Economic Studies 81 (2): 608–50. Blair, Graeme, Jasper Cooper, Alexander Coppock, Macartan Humphreys, and Luke Sonnet. 2021. Estimatr: Fast Estimators for Design-Based Inference. https://CRAN.R-project.org/package=estimatr. Chernozhukov, Victor, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, and James Robins. 2018. “Double/Debiased Machine Learning for Treatment and Structural Parameters: Double/Debiased Machine Learning.” The Econometrics Journal 21 (1). Friedman, Jerome, Trevor Hastie, Rob Tibshirani, Balasubramanian Narasimhan, Kenneth Tay, and Noah Simon. 2021. Glmnet: Lasso and Elastic-Net Regularized Generalized Linear Models. https://CRAN.R-project.org/package=glmnet. James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. An Introduction to Statistical Learning. Vol. 112. Springer. Kleiber, Christian, and Achim Zeileis. 2008. Applied Econometrics with R. New York: Springer-Verlag. https://CRAN.R-project.org/package=AER. Robinson, Peter. 1988. “Root-n-Consistent Semiparametric Regression.” Econometrica 56 (4): 931–54. Spindler, Martin, Victor Chernozhukov, and Christian Hansen. 2019. Hdm: High-Dimensional Metrics. https://CRAN.R-project.org/package=hdm. Wickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain Francois, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686. Wickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen, Kohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani, and Dewey Dunnington. 2020. Ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics. https://CRAN.R-project.org/package=ggplot2. Wickham, Hadley, Romain Francois, Lionel Henry, and Kirill Muller. 2021. Dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr. Wright, Marvin N., Stefan Wager, and Philipp Probst. 2020. Ranger: A Fast Implementation of Random Forests. https://github.com/imbs-hl/ranger. "]]
