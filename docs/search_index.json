[["prediction.html", "Chapter 4 予測 4.1 問題設定 4.2 パッケージ &amp; データ 4.3 事前準備 4.4 OLS 4.5 LASSO 4.6 Ridge 4.7 Tree 4.8 Random Forest 4.9 Stacking", " Chapter 4 予測 データと同じ母集団から新しくランダムサンプリングされ、\\(X\\)のみが観察できるサンプルについて\\(Y\\)の値を予測する。 データ分割を用いたモデル評価を行う Chapter 4.1 : 予測問題の論点を紹介 Chapter 4.3 : データ分割 Chapter 4.4 : 線形モデルをOLSで推定する手法を紹介 Chapter 4.5-4.6 : 線形モデルを罰則付き回帰(LASSO, Ridge)で推定する手法を紹介 Chapter 4.7 : 予測木モデルを推定する手法を紹介 Chapter 4.8 : 予測木モデルをモデル平均加法(Bagging, Random Forest)で推定する手法を紹介 4.1 問題設定 事前に定義する損失関数の期待値を最小化するような、予測関数\\(f(X)\\)の推定を目指す。 本ページではMean squared error(MSE)を損失関数として用いるケースを紹介する。所与の\\(f(X)\\)、母分布に従う確率変数\\(Y,X\\)についてMSEは以下のように定義される。 \\[MSE = E_{X,Y}[(Y_i-f(X_i))^2]\\] 一般にMSEは以下のように書き換えられる。 \\[MSE = \\underbrace{E_{X,Y}[(Y_i-\\bar{Y}(X_i))^2]}_{Irreducible\\ error}+\\underbrace{E_{X,Y}[(\\bar{Y}(X_i)-f(X_i))^2]}_{Reducible\\ error}\\] ただし\\(\\bar{Y}(X_i)=E[Y_i|X_i]\\)。 上記式から以下が確認できる。 最善の予測関数 \\(\\iff\\) Reducible error \\(=0\\) \\(\\iff\\) 条件付き母平均\\(\\bar{Y}(X_i)\\) 最善の予測関数のもとでも削減不可能なエラー(Irreducible error)が存在 予測関数の推定 \\(=\\) Reducible errorの削減 \\(=\\) 条件付き母平均との乖離(MSE)の削減 4.1.1 Bias-Variance tradeoff 実際の\\(f(X_i)\\)はランダムサンプリングされたデータから推定される。このためデータの入手前の段階では、確率分布を持つ。 Reducible errorは一般に以下のように書き換えられます。 \\[E_{Y,X,f(X)}[(\\bar{Y}(X_i)-f(X_i))^2]\\] \\[=\\underbrace{(E_{Y,X,f(X)}[\\bar{Y}(X_i)-\\bar{f}(X_i)])^2}_{Bias}+\\underbrace{E_{Y,X,f(X)}[(\\bar{f}(X_i)-f(X_i))^2]}_{Variance}.\\] ただし\\(\\bar{f}(X_i)=f(X_i)\\)。 上記式は推定される予測関数が平均的にどの程度条件付き母平均を近似できているのか(Biasがどの程度小さいのか)だけでなく、予測関数の分布がどの程度散らばっているのか、についても考慮する必要性を示す。 母平均\\(\\bar Y(X_i)\\)が単純な既知の関数形に従い、かつサンプルサイズが大きい場合、OLS推定された\\(f(X_i)\\)は\\(Bias=0\\)かつ小さなVarianceを達成する。 しかしながら社会科学における多くの応用においては、\\(\\bar Y(X_i)\\)は未知かつ複雑であることが予想され、その複雑さに対してサンプルサイズが小さいことが想定される。 このような状況では、深刻なBias-Variance tradeoffに直面する。 少ないパラメータ（短い回帰式、少ないサブサンプル分割）を推定する場合、大きなBiasを持つ 多くのパラメータ（長い回帰式、多いサブサンプル分割）を推定する場合、大きなVarianceを持つ。 Bias-variance tradeoffを分析者が解くことは一般に困難であり、@ref{LASSO} - ?? で紹介するLASSO/Ridge/Random Forestなどの手法は、bias-variance問題をよりデータ主導型かつ現実的な計算時間の手法で解決することを目指す。 4.2 パッケージ &amp; データ 利用するパッケージ library(tidyverse) library(AER) library(SuperLearner) # 機械学習を実装するメタパッケージ library(rpart.plot) # 予測木の可視化 データ 元データを結果変数、予測変数データに分割する必要がある data(&quot;NMES1988&quot;) Y &lt;- NMES1988$emergency # 結果変数 X &lt;- select(NMES1988, -emergency ) # 予測変数 set.seed(123) 4.3 事前準備 ここでは5個のデータに分割する。 group &lt;- sample(1:5, # 1から5までの数字を発生される size = length(Y), # サンプルサイズと同数発生される replace = TRUE) # 同じ数字が発生することを許容する 第1データをテストデータ、2－5データを訓練データとして使用する 4.4 OLS 線形予測関数\\(f(X)=\\beta_0 + \\beta_1X_1+...+\\beta_LX_L\\)を想定 \\(\\beta_0,...,\\beta_L\\)を最小二乗法にて推定 \\[\\min\\sum_i (Y_i-f(X))^2\\] SuperLeanerによる推定と推定されたモデル fit &lt;- SuperLearner(Y = Y[group != 1], X = X[group != 1,], SL.library = c(&quot;SL.lm&quot;) ) # 推定 coef(fit$fitLibrary$SL.lm_All$object) # 係数値の表示 ## (Intercept) visits nvisits ovisits novisits ## 0.1377112926 0.0036552347 0.0009715062 0.0032976320 -0.0031689963 ## hospital healthpoor healthexcellent chronic adllimited ## 0.4037474005 0.0969485276 -0.0307438996 0.0304625940 0.0685460489 ## regionnortheast regionmidwest regionwest age afamyes ## 0.0052059323 -0.0181151135 0.0287424583 0.0003025273 0.0327714591 ## gendermale marriedyes school income employedyes ## -0.0048794625 -0.0222767601 -0.0085525318 0.0042461305 0.0273419261 ## insuranceyes medicaidyes ## -0.0117301335 0.0295738407 予測値の計算 Y.pred &lt;- predict(fit, X)$pred テストデータへの適合 mean((Y - Y.pred)[group == 1]^2)/var(Y[group == 1]) ## [1] 0.7421171 訓練データ mean((Y - Y.pred)[group != 1]^2)/var(Y[group != 1]) ## [1] 0.7545414 4.5 LASSO LASSO推定：線形モデルを以下の最適化問題の解として推定 \\[\\min\\sum_i (Y_i-f(X_i))^2+\\underbrace{\\lambda\\sum_l|\\beta_l|}_{Penalty\\ term}\\] \\(\\lambda\\) : チューニングパラメタ、Cross-validationを用いて設定可能 glmentパッケージ(Friedman et al. 2021)を利用 glmnetはdata.frameを直接の入力できず、matrix(vector)に変換する必要がある fit &lt;- SuperLearner(Y = Y[group != 1], X = X[group != 1,], SL.library = c(&quot;SL.glmnet&quot;) ) coef(fit$fitLibrary$SL.glmnet_All$object) ## 23 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## s1 ## (Intercept) 0.2267243 ## visits . ## nvisits . ## ovisits . ## novisits . ## hospital 0.1373981 ## healthpoor . ## healthaverage . ## healthexcellent . ## chronic . ## adllimited . ## regionnortheast . ## regionmidwest . ## regionwest . ## age . ## afamyes . ## gendermale . ## marriedyes . ## school . ## income . ## employedyes . ## insuranceyes . ## medicaidyes . テストデータへの適合 mean((Y - Y.pred)[group == 1]^2)/var(Y[group == 1]) ## [1] 0.7421171 訓練データ mean((Y - Y.pred)[group != 1]^2)/var(Y[group != 1]) ## [1] 0.7545414 4.6 Ridge Ridge推定：線形モデルを以下の最適化問題の解として推定 \\[\\min\\sum_i (Y_i-f(X_i))^2+\\underbrace{\\lambda\\sum_l(\\beta_l)^2}_{Penalty\\ term}\\] \\(\\lambda\\) : チューニングパラメタ、Cross-validationを用いて設定可能 引き続きglmentパッケージ(Friedman et al. 2021)を利用 learners = create.Learner(&quot;SL.glmnet&quot;, params = list(alpha = 0)) # glmnetのalphaを0（Ridge推定）に設定 fit &lt;- SuperLearner(Y = Y, X = X, SL.library = c(learners$names) ) coef(fit$fitLibrary$SL.glmnet_1_All$object) ## 23 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## s1 ## (Intercept) 1.393051e-01 ## visits 3.545363e-03 ## nvisits 1.746575e-03 ## ovisits 2.521944e-03 ## novisits -1.055793e-05 ## hospital 1.559993e-01 ## healthpoor 8.134034e-02 ## healthaverage -3.493675e-02 ## healthexcellent -4.565236e-02 ## chronic 2.423562e-02 ## adllimited 6.203800e-02 ## regionnortheast -3.103490e-03 ## regionmidwest -8.315759e-03 ## regionwest 6.909235e-03 ## age 9.151894e-03 ## afamyes 1.956992e-02 ## gendermale 2.357971e-04 ## marriedyes -1.373979e-02 ## school -2.850516e-03 ## income -2.623278e-04 ## employedyes 4.012830e-03 ## insuranceyes -1.372911e-02 ## medicaidyes 3.394999e-02 テストデータへの適合 mean((Y - Y.pred)[group == 1]^2)/var(Y[group == 1]) ## [1] 0.7421171 訓練データ mean((Y - Y.pred)[group != 1]^2)/var(Y[group != 1]) ## [1] 0.7545414 4.7 Tree 予測木をrpartパッケージにより推定する 推定法は以下の通り ある変数\\(X\\)のある閾値\\(\\bar X\\)において、サンプルを分割する 分割後のサブサンプル平均を暫定的予測値とする 予測値と訓練データにおける結果変数の値の乖離(MSE)を最小にするように、分割に用いる変数と閾値を決定 １回目の分割結果を所与として、２回目の分割を決定。変数と閾値は１回目と同様の基準で決定。 以上を繰り返す 推定された予測木について、pruningを実行 fit &lt;- SuperLearner(Y = Y, X = X, SL.library = c(&quot;SL.rpartPrune&quot;) ) rpart.plot(fit$fitLibrary$SL.rpartPrune_All$object) # 予測木の可視化 テストデータへの適合 mean((Y - Y.pred)[group == 1]^2)/var(Y[group == 1]) ## [1] 0.7421171 訓練データ mean((Y - Y.pred)[group != 1]^2)/var(Y[group != 1]) ## [1] 0.7545414 4.8 Random Forest rangerパッケージ(Wright, Wager, and Probst 2021)を利用 fit &lt;- SuperLearner(Y = Y, X = X, SL.library = c(&quot;SL.ranger&quot;) ) テストデータへの適合 mean((Y - Y.pred)[group == 1]^2)/var(Y[group == 1]) ## [1] 0.7421171 訓練データ mean((Y - Y.pred)[group != 1]^2)/var(Y[group != 1]) ## [1] 0.7545414 4.9 Stacking 交差検証法を用いて、複数の推定アルゴリズムの性能を比較する 複数の予測モデルを線型結合した予測モデル（Super learner: Van der Laan, Polley, and Hubbard (2007)）も推定し、性能評価 learners = create.Learner(&quot;SL.glmnet&quot;, params = list(alpha = 0)) # Ridge推定の定義 fit &lt;- CV.SuperLearner(X = X, Y = Y, SL.library = c(&quot;SL.mean&quot;, &quot;SL.lm&quot;, &quot;SL.glmnet&quot;, &quot;SL.rpartPrune&quot;, &quot;SL.ranger&quot;, learners$names ) ) # モデル推定 &amp; 交差検証 summary(fit) ## ## Call: ## CV.SuperLearner(Y = Y, X = X, SL.library = c(&quot;SL.mean&quot;, &quot;SL.lm&quot;, &quot;SL.glmnet&quot;, ## &quot;SL.rpartPrune&quot;, &quot;SL.ranger&quot;, learners$names)) ## ## Risk is based on: Mean Squared Error ## ## All risk estimates are based on V = 10 ## ## Algorithm Ave se Min Max ## Super Learner 0.38212 0.038569 0.25563 0.69386 ## Discrete SL 0.37908 0.037237 0.25803 0.65788 ## SL.mean_All 0.49535 0.052140 0.33259 0.96410 ## SL.lm_All 0.37958 0.036531 0.26094 0.64812 ## SL.glmnet_All 0.37895 0.037248 0.25803 0.65788 ## SL.rpartPrune_All 0.42689 0.043130 0.25465 0.76707 ## SL.ranger_All 0.39119 0.040852 0.26127 0.76885 ## SL.glmnet_1_All 0.37935 0.036874 0.25972 0.65549 References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
