[["index.html", "Rによる比較・予測・因果推論入門 ver0.1 はじめに", " Rによる比較・予測・因果推論入門 ver0.1 川田恵介 2021-05-04 はじめに 本ページでは、定量的な比較、（反実仮想）因果推定、予測分析をRによって行う方法を紹介します。 データインポート、整理、可視化を行う関数群を統合的に提供するtidyverseパッケージ(Wickham et al. 2019)の利用を前提にします。 AERパッケージ(Kleiber and Zeileis 2008)に含まれるNMES1988を例として使用します。 無料で公開されている有力な参考文献として、以下を推薦します。 機械学習を用いた予測：James et al. (2013) 公開ページ 日本語によるR入門：私たちのR: ベストプラクティスの探究 tidyverseパッケージの利用： 公式ページ "],["intro.html", "Chapter 1 準備", " Chapter 1 準備 オフライン環境の整備 Rのインストール R studioのインストール オンライン環境の整備 R cloudへの登録 "],["問題意識.html", "Chapter 2 問題意識 2.1 予測 2.2 比較 2.3 因果効果", " Chapter 2 問題意識 経済学におけるデータ分析の大部分は、複数の変数間での関係性の理解・利用を目的とします。 ここではある結果変数\\(Y\\)と独立変数（群）\\(X=X_1,...,X_L\\)の関係性に焦点を当てます。 より具体的な目標は大きく（予測）\\(Y\\)の予測関数の推定、（比較）異なる\\(X\\)間での\\(Y\\)の比較、（因果効果）\\(X\\)の変化が\\(Y\\)に与える因果効果の推定、に大別できます。 \\(Y\\)と\\(X\\)がともに観察でき、関心のある母集団からランダムサンプリングされたデータが入手出来ているとします。 2.1 予測 Chapter 5 の背後にある問題意識・方針を紹介します。 問題設定：データと同じ母集団から新しくランダムサンプリングされ、\\(X\\)のみが観察できるサンプルについて\\(Y\\)の値を予測します。 2.1.1 損失関数 具体的には、事前に定義する損失関数の期待値を最小化するような、予測関数\\(f(X)\\)の推定をめざします。 以下ではMean squared errorを損失関数として用います。 所与の\\(f(X)\\)、母分布に従う確率変数\\(Y,X\\)についてMSEは以下のように定義されます。 \\[MSE = E_{X,Y}[(Y_i-f(X_i))^2]\\] 一般にMSEは以下のように書き換えられます。 \\[MSE = \\underbrace{E_{X,Y}[(Y_i-\\bar{Y}(X_i))^2]}_{Irreducible\\ error}+\\underbrace{E_{X,Y}[(\\bar{Y}(X_i)-f(X_i))^2]}_{Reducible\\ error}\\] ただし\\(\\bar{Y}(X_i)=E[Y_i|X_i]\\)。 上記式から 最善の予測関数は条件付き母平均\\(\\bar{Y}(X_i)\\) 最善の予測関数のもとでも削減不可能なエラー(Irreducible error)が存在 予測関数の推定 \\(=\\) Reducible errorの削減 \\(=\\) 条件付き母平均との乖離(MSE)の削減 2.1.2 Bias-Variance tradeoff 実際の\\(f(X_i)\\)はランダムサンプリングされたデータから推定される必要があり、実際には確率的に決定される。 Reducible errorは一般に以下のように書き換えられる。 \\[E_{Y,X,f(X)}[(\\bar{Y}(X_i)-f(X_i))^2]\\] \\[=\\underbrace{(E_{Y,X,f(X)}[\\bar{Y}(X_i)-\\bar{f}(X_i)])^2}_{Bias}+\\underbrace{E_{Y,X,f(X)}[(\\bar{f}(X_i)-f(X_i))^2]}_{Variance}.\\] ただし\\(\\bar{f}(X_i)=f(X_i)\\)。 母平均\\(\\bar Y(X_i)\\)が単純な既知の関数形に従い、かつサンプルサイズが大きい場合、OLS推定された\\(f(X_i)\\)は\\(Bias=0\\)かつ小さいVarianceを達成する。 社会科学における応用においては、\\(\\bar Y(X_i)\\)は未知かつ複雑であることが想定され、その複雑さに対してサンプルサイズが小さいことが想定される。 OLSやサブサンプル平均により推定された予測モデルは、Bias-Variance tradeoffに直面する 少ないパラメータ（短い回帰式、少ないサブサンプル分割）を推定する場合、大きなBiasを持つ 多くのパラメータ（長い回帰式、多いサブサンプル分割）を推定する場合、大きなVarianceを持つ。 Bias-variance tradeoffを分析者が解くこと（最善のモデル設定を行うこと）は困難 Chapter 5 で紹介するLASSO/Ridge/Random Forestなどの手法は、よりデータ主導型のアプローチを現実的な計算時間で行う。 2.2 比較 Chapter 6 の背後にある問題意識・方針を紹介します。 2.3 因果効果 Chapter 6 の背後にある問題意識・方針を紹介します。 "],["データ整備.html", "Chapter 3 データ整備 3.1 新しい変数の作成: mutate (tidyverse) 3.2 変数の限定: select (tidyverse) 3.3 サンプルの除外:filter (tidyverse)", " Chapter 3 データ整備 AERパッケージに含まれるNMES1988を利用 data(&quot;NMES1988&quot;, package = &quot;AER&quot;) raw &lt;- NMES1988 tidyverseパッケージに含まれるdplyrパッケージ(Wickham et al. 2021)は、有用な関数を提供 library(tidyverse) 3.1 新しい変数の作成: mutate (tidyverse) mutate関数の利用 df &lt;- mutate(raw, square_age = age^2, log_age = log(age), total_visit = visits + nvisits + ovisits + novisits ) 3.2 変数の限定: select (tidyverse) select関数の利用 df &lt;- select(raw, age, visits ) 特定の変数の除外 df &lt;- select(raw, -age, -visits ) 3.3 サンプルの除外:filter (tidyverse) filter関数の利用 df &lt;- filter(raw, visits &gt;= 5 ) "],["可視化.html", "Chapter 4 可視化 4.1 Y=連続、Xカテゴリ 4.2 Y=連続、X=連続", " Chapter 4 可視化 tidyverseに含まれるggplot2パッケージ(Wickham et al. 2020)を利用 library(tidyverse) 4.1 Y=連続、Xカテゴリ 4.1.1 ヒストグラム: geom_histogram (tidyverse) 医療機関の利用回数 ggplot(raw, aes(x = visits) ) + geom_histogram() メディケイド保有別の利用回数 ggplot(raw, aes(x = visits, fill = medicaid) ) + geom_histogram(position = &quot;identity&quot;, alpha = 0.5) 男女・メディケイド保有別の利用回数 ggplot(raw, aes(x = visits, fill = medicaid) ) + geom_histogram(position = &quot;identity&quot;, alpha = 0.5 ) + facet_wrap(~ gender) 4.1.2 密度: geom_density (tidyverse) 男女・メディケイド保有別の利用回数 ggplot(raw, aes(x = visits, fill = medicaid) ) + geom_density(position = &quot;identity&quot;, alpha = 0.5 ) + facet_wrap(~ gender) 4.1.3 Boxplot: geom_boxplot (tidyverse) ggplot(raw, aes(y = visits, x = medicaid) ) + geom_boxplot() 4.2 Y=連続、X=連続 4.2.1 散布図: geom_point (tidyverse) 散布図：連続変数間の関係性を可視化する図 ggplot(raw, aes(x = income, y = visits) ) + geom_point() サンプルサイズが大きくなると機能しない 4.2.2 ヒートマップ: geom_bin2d (tidyverse) 代替案はヒートマップ ggplot(raw, aes(x = income, y = visits) ) + geom_bin2d() "],["prediction.html", "Chapter 5 予測関数の推定 5.1 データの導入 5.2 データ分割 5.3 OLS 5.4 LASSO/Ridge 5.5 Random Forest/Bagging", " Chapter 5 予測関数の推定 5.1 データの導入 data(&quot;NMES1988&quot;, package = &quot;AER&quot;) raw &lt;- NMES1988 5.2 データ分割 ここでは5個のデータに分割する。 group &lt;- sample(1:5, size = nrow(raw), replace = TRUE) 5.3 OLS 線形予測関数\\(f(X)=\\beta_0 + \\beta_1X_1+...+\\beta_LX_L\\)を仮定し、最小二乗法にて推定する。 fit &lt;- lm(visits ~ ., data = raw[group != 1,]) coef(fit) ## (Intercept) nvisits ovisits novisits emergency ## 2.41761449 0.21866924 0.00996545 0.11282753 0.01646699 ## hospital healthpoor healthexcellent chronic adllimited ## 1.54062092 1.94459674 -1.60584479 0.90187096 -0.09692064 ## regionnortheast regionmidwest regionwest age afamyes ## 0.41635279 -0.29554163 0.43170033 -0.16865692 -0.35617003 ## gendermale marriedyes school income employedyes ## -0.39881922 -0.12918912 0.12461058 -0.05749529 0.41508018 ## insuranceyes medicaidyes ## 1.47165757 1.66355342 予測値の導出 Y.hat &lt;- predict(fit,raw) テストデータへの適合 mean((raw$visits - Y.hat)[group == 1]^2) ## [1] 34.51723 訓練データへの適合 mean((raw$visits - Y.hat)[group != 1]^2) ## [1] 38.61726 5.4 LASSO/Ridge glmentパッケージ(Friedman et al. 2021)を利用 5.5 Random Forest/Bagging rangerパッケージ(Wright, Wager, and Probst 2020)を利用 "],["unique.html", "Chapter 6 単一のパラメータの推定 6.1 データ 6.2 部分線形モデルに基づく推定", " Chapter 6 単一のパラメータの推定 （条件付き）平均差を推定する。 点推定だけでなく、信頼区間も推定する。 6.1 データ library(tidyverse) data(&quot;NMES1988&quot;, package = &quot;AER&quot;) raw &lt;- NMES1988 6.2 部分線形モデルに基づく推定 部分線形モデルに関心のあるパラメータを埋め込む \\[E[Y|D=d,X=x]=\\underbrace{\\tau}_{Interest\\ parameter}\\times d+\\underbrace{f(x)}_{Nuisance\\ function}\\] 6.2.1 OLS by lm_robust (estimatr) \\(\\tau(x)=\\tau,f(x)=\\beta_0+\\beta_1x_1+...+\\beta_Lx_L\\)と特定化 サンプル内MSEを最大化するように推定 robust standard errorを計算するためにestimatrパッケージ(Blair et al. 2021)を利用 library(estimatr) lm_robust関数で推定 lm_robust(visits ~ insurance + age + gender + school + income + employed + region + afam + married, data = raw) ## Estimate Std. Error t value Pr(&gt;|t|) CI Lower ## (Intercept) 4.32086605 1.30928537 3.3001713 9.739642e-04 1.75400683 ## insuranceyes 0.96591490 0.24785140 3.8971533 9.877947e-05 0.48000123 ## age 0.04293421 0.15977094 0.2687235 7.881551e-01 -0.27029737 ## gendermale -0.46726544 0.22050124 -2.1191057 3.413748e-02 -0.89955902 ## school 0.08475793 0.03108035 2.7270588 6.415540e-03 0.02382479 ## income -0.04678801 0.03712934 -1.2601357 2.076873e-01 -0.11958023 ## employedyes -0.34186375 0.42407955 -0.8061312 4.202108e-01 -1.17327342 ## regionnortheast 0.34814737 0.30392728 1.1454957 2.520663e-01 -0.24770327 ## regionmidwest -0.40583622 0.25526934 -1.5898354 1.119439e-01 -0.90629278 ## regionwest 0.57163030 0.30418463 1.8792215 6.028038e-02 -0.02472489 ## afamyes -0.39294341 0.34701206 -1.1323624 2.575439e-01 -1.07326195 ## marriedyes -0.29559842 0.23244624 -1.2716851 2.035523e-01 -0.75131020 ## CI Upper DF ## (Intercept) 6.88772528 4394 ## insuranceyes 1.45182856 4394 ## age 0.35616578 4394 ## gendermale -0.03497187 4394 ## school 0.14569108 4394 ## income 0.02600421 4394 ## employedyes 0.48954591 4394 ## regionnortheast 0.94399802 4394 ## regionmidwest 0.09462034 4394 ## regionwest 1.16798549 4394 ## afamyes 0.28737512 4394 ## marriedyes 0.16011336 4394 発展:推計結果表 tidy関数により推定結果data.frameに変化することで、kable関数(knitrパッケージ)による推計結果表の整形、geom_pointrange関数による可視化が可能 点推定値(estimate)、標準誤差(std.error)のみを残した推計結果表 library(knitr) library(tidyverse) fit &lt;- lm_robust(visits ~ insurance + age + gender + school + income + employed + region + afam + married, data = raw) fit &lt;- tidy(fit) fit &lt;- select(fit, term, estimate, std.error) kable(fit, digits = 2) term estimate std.error (Intercept) 4.32 1.31 insuranceyes 0.97 0.25 age 0.04 0.16 gendermale -0.47 0.22 school 0.08 0.03 income -0.05 0.04 employedyes -0.34 0.42 regionnortheast 0.35 0.30 regionmidwest -0.41 0.26 regionwest 0.57 0.30 afamyes -0.39 0.35 marriedyes -0.30 0.23 fit &lt;- filter(fit, term == &quot;insuranceyes&quot;) kable(fit, digits = 2) term estimate std.error insuranceyes 0.97 0.25 発展:Dot-and-Whisker plotによる可視化 Dot-and-Whisker図により点推定量と信頼区間を可視化 fit &lt;- lm_robust(visits ~ insurance + age + gender + school + income + employed + region + afam + married, data = raw) fit &lt;- tidy(fit) fit &lt;- filter(fit, term != &quot;(Intercept)&quot;) ggplot(fit, aes(y = term, x = estimate, xmin = conf.low, xmax = conf.high)) + geom_pointrange() fit &lt;- filter(fit, term == &quot;insuranceyes&quot;) ggplot(fit, aes(y = term, x = estimate, xmin = conf.low, xmax = conf.high)) + geom_pointrange() + geom_vline(xintercept = 0) 6.2.2 ロビンソン変換(Robinson 1988) 部分線形モデルをロビンソン変換(Robinson 1988) \\[Y_i-\\underbrace{E[Y_i|X_i]}_{Nuisance\\ term}=\\tau\\times [D_i-\\underbrace{E[D_i|X_i]}_{Nuisance\\ term}]+u_i\\] \\(E[Y_i|X_i],E[D_i|X_i]\\)を予測関数として推定し、予測誤差間を単回帰すればよい 実際には\\(E[Y_i|X_i],E[D_i|X_i]\\)は未知の関数なので何らかの方法で推定する必要がある。関数の推定なので予測の手法が適用できる。 6.2.2.1 Double selection: rlassoEffect (hdm) 2重選択法(Belloni, Chernozhukov, and Hansen 2014)を紹介 LASSOにより\\(Y_i,D_i\\)の両方あるいはどちらか一方を予測する上でrelevantな\\(X^c\\)を特定しコントロールする \\(Y_i,D_i\\)どちらの予測にもrelevantではない変数は除外する hdmパッケージ(Spindler, Chernozhukov, and Hansen 2019)を利用 library(hdm) Y &lt;- raw$visits D &lt;- if_else(raw$insurance == &quot;yes&quot;, 1, 0) X &lt;- model.matrix(~ - 1+ age + gender + school + income + employed + region + afam + married, raw) fit &lt;- rlassoEffect(x = X, y = Y, d = D, method = &quot;double selection&quot;) 推定結果 summary(fit) ## [1] &quot;Estimates and significance testing of the effect of target variables&quot; ## Estimate. Std. Error t value Pr(&gt;|t|) ## d1 0.9597 0.2480 3.87 0.000109 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 選択されたコントロール変数 fit$selection.index ## age genderfemale gendermale school income ## FALSE FALSE FALSE TRUE TRUE ## employedyes regionnortheast regionmidwest regionwest afamyes ## FALSE FALSE TRUE FALSE TRUE ## marriedyes ## TRUE 6.2.2.2 Double Machine Learning (DoubleML) Double Machine Learning法(Chernozhukov et al. 2018)を紹介 なんらかの方法（例、OLS、ランダムフォレスト、LASSO）で\\(E[Y|X],E[D|X]\\)の予測関数\\(f_Y(X),f_D(X)\\)を推定し、予測誤差を単回帰 DoubleMLパッケージ(Bach et al. 2021)を利用 library(DoubleML) library(mlr3) library(mlr3learners) library(data.table) learner &lt;- lrn(&quot;regr.ranger&quot;, num.trees = 100) # Require bigger num.trees in practice ml_g &lt;- learner$clone() ml_m &lt;- learner$clone() obj_dml_data &lt;- double_ml_data_from_matrix(X = X, y = Y, d = D) dml_plr_obj &lt;- DoubleMLPLR$new(obj_dml_data, ml_g, ml_m, dml_procedure=&quot;dml1&quot;, n_rep = 3) dml_plr_obj$fit() ## INFO [23:52:54.525] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_g&#39; (iter 2/5) ## INFO [23:52:55.759] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_g&#39; (iter 1/5) ## INFO [23:52:56.057] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_g&#39; (iter 4/5) ## INFO [23:52:56.386] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_g&#39; (iter 5/5) ## INFO [23:52:56.680] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_g&#39; (iter 3/5) ## INFO [23:52:57.555] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_m&#39; (iter 5/5) ## INFO [23:52:57.862] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_m&#39; (iter 1/5) ## INFO [23:52:58.132] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_m&#39; (iter 2/5) ## INFO [23:52:58.412] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_m&#39; (iter 3/5) ## INFO [23:52:58.687] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_m&#39; (iter 4/5) ## INFO [23:52:59.236] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_g&#39; (iter 4/5) ## INFO [23:52:59.499] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_g&#39; (iter 2/5) ## INFO [23:52:59.742] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_g&#39; (iter 1/5) ## INFO [23:52:59.994] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_g&#39; (iter 5/5) ## INFO [23:53:00.246] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_g&#39; (iter 3/5) ## INFO [23:53:00.582] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_m&#39; (iter 1/5) ## INFO [23:53:00.786] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_m&#39; (iter 4/5) ## INFO [23:53:01.002] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_m&#39; (iter 3/5) ## INFO [23:53:01.212] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_m&#39; (iter 5/5) ## INFO [23:53:01.432] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_m&#39; (iter 2/5) ## INFO [23:53:01.714] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_g&#39; (iter 4/5) ## INFO [23:53:01.945] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_g&#39; (iter 5/5) ## INFO [23:53:02.164] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_g&#39; (iter 3/5) ## INFO [23:53:02.405] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_g&#39; (iter 1/5) ## INFO [23:53:02.629] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_g&#39; (iter 2/5) ## INFO [23:53:03.113] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_m&#39; (iter 3/5) ## INFO [23:53:03.323] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_m&#39; (iter 1/5) ## INFO [23:53:03.508] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_m&#39; (iter 5/5) ## INFO [23:53:03.698] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_m&#39; (iter 4/5) ## INFO [23:53:03.898] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_m&#39; (iter 2/5) print(dml_plr_obj) ## ================= DoubleMLPLR Object ================== ## ## ## ------------------ Data summary ------------------ ## Outcome variable: y ## Treatment variable(s): d ## Covariates: X1, X2, X3, X4, X5, X6, X7, X8, X9, X10, X11 ## Instrument(s): ## No. Observations: 4406 ## ## ------------------ Score &amp; algorithm ------------------ ## Score function: partialling out ## DML algorithm: dml1 ## ## ------------------ Machine learner ------------------ ## ml_g: regr.ranger ## ml_m: regr.ranger ## ## ------------------ Resampling ------------------ ## No. folds: 5 ## No. repeated sample splits: 3 ## Apply cross-fitting: TRUE ## ## ------------------ Fit summary ------------------ ## [1] &quot;Estimates and significance testing of the effect of target variables&quot; ## Estimate. Std. Error t value Pr(&gt;|t|) ## d 1.1712 0.2634 4.446 8.73e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 "],["references.html", "References", " References Bach, Philipp, Victor Chernozhukov, Malte S. Kurz, and Martin Spindler. 2021. DoubleML: Double Machine Learning in r. https://CRAN.R-project.org/package=DoubleML. Belloni, Alexandre, Victor Chernozhukov, and Christian Hansen. 2014. “Inference on Treatment Effects After Selection Among High-Dimensional Controls.” The Review of Economic Studies 81 (2): 608–50. Blair, Graeme, Jasper Cooper, Alexander Coppock, Macartan Humphreys, and Luke Sonnet. 2021. Estimatr: Fast Estimators for Design-Based Inference. https://CRAN.R-project.org/package=estimatr. Chernozhukov, Victor, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, and James Robins. 2018. “Double/Debiased Machine Learning for Treatment and Structural Parameters: Double/Debiased Machine Learning.” The Econometrics Journal 21 (1). Friedman, Jerome, Trevor Hastie, Rob Tibshirani, Balasubramanian Narasimhan, Kenneth Tay, and Noah Simon. 2021. Glmnet: Lasso and Elastic-Net Regularized Generalized Linear Models. https://CRAN.R-project.org/package=glmnet. James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. An Introduction to Statistical Learning. Vol. 112. Springer. Kleiber, Christian, and Achim Zeileis. 2008. Applied Econometrics with R. New York: Springer-Verlag. https://CRAN.R-project.org/package=AER. Robinson, Peter. 1988. “Root-n-Consistent Semiparametric Regression.” Econometrica 56 (4): 931–54. Spindler, Martin, Victor Chernozhukov, and Christian Hansen. 2019. Hdm: High-Dimensional Metrics. https://CRAN.R-project.org/package=hdm. Wickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain Francois, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686. Wickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen, Kohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani, and Dewey Dunnington. 2020. Ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics. https://CRAN.R-project.org/package=ggplot2. Wickham, Hadley, Romain Francois, Lionel Henry, and Kirill Muller. 2021. Dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr. Wright, Marvin N., Stefan Wager, and Philipp Probst. 2020. Ranger: A Fast Implementation of Random Forests. https://github.com/imbs-hl/ranger. "]]
