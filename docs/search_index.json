[["index.html", "Rによる比較・予測・因果推論入門 ver0.1 はじめに", " Rによる比較・予測・因果推論入門 ver0.1 川田恵介 2021-05-04 はじめに 本ページでは、定量的な比較、（反実仮想）因果推定、予測分析をRによって行う方法を紹介します。 データインポート、整理、可視化を行う関数群を統合的に提供するtidyverseパッケージ(Wickham et al. 2019)の利用を前提にします。 AERパッケージ(Kleiber and Zeileis 2008)に含まれるNMES1988を例として使用します。 無料で公開されている有力な参考文献として、以下を推薦します。 機械学習を用いた予測：James et al. (2013) 公開ページ 日本語によるR入門：私たちのR: ベストプラクティスの探究 tidyverseパッケージの利用： 公式ページ "],["intro.html", "Chapter 1 準備", " Chapter 1 準備 オフライン環境の整備 Rのインストール R studioのインストール オンライン環境の整備 R cloudへの登録 "],["識別.html", "Chapter 2 識別 2.1 予測 2.2 比較 2.3 因果効果", " Chapter 2 識別 経済学におけるデータ分析の大部分は、複数の変数間での関係性の理解・利用を目的とします。 ここではある結果変数\\(Y\\)と独立変数（群）\\(X=X_1,...,X_L\\)の関係性に焦点を当てます。 より具体的な目標は大きく（予測）\\(Y\\)の予測、（比較）異なる\\(X\\)間での\\(Y\\)の比較、（因果効果）\\(X\\)の変化が\\(Y\\)に与える因果効果 2.1 予測 2.2 比較 2.3 因果効果 "],["データ整備.html", "Chapter 3 データ整備 3.1 新しい変数の作成: mutate (tidyverse) 3.2 変数の限定: select (tidyverse) 3.3 サンプルの除外:filter (tidyverse)", " Chapter 3 データ整備 AERパッケージに含まれるNMES1988を利用 data(&quot;NMES1988&quot;, package = &quot;AER&quot;) raw &lt;- NMES1988 tidyverseパッケージに含まれるdplyrパッケージ(Wickham et al. 2021)は、有用な関数を提供 library(tidyverse) 3.1 新しい変数の作成: mutate (tidyverse) mutate関数の利用 df &lt;- mutate(raw, square_age = age^2, log_age = log(age), total_visit = visits + nvisits + ovisits + novisits ) 3.2 変数の限定: select (tidyverse) select関数の利用 df &lt;- select(raw, age, visits ) 特定の変数の除外 df &lt;- select(raw, -age, -visits ) 3.3 サンプルの除外:filter (tidyverse) filter関数の利用 df &lt;- filter(raw, visits &gt;= 5 ) "],["可視化.html", "Chapter 4 可視化 4.1 Y=連続、Xカテゴリ 4.2 Y=連続、X=連続", " Chapter 4 可視化 tidyverseに含まれるggplot2パッケージ(Wickham et al. 2020)を利用 library(tidyverse) 4.1 Y=連続、Xカテゴリ 4.1.1 ヒストグラム: geom_histogram (tidyverse) 医療機関の利用回数 ggplot(raw, aes(x = visits) ) + geom_histogram() メディケイド保有別の利用回数 ggplot(raw, aes(x = visits, fill = medicaid) ) + geom_histogram(position = &quot;identity&quot;, alpha = 0.5) 男女・メディケイド保有別の利用回数 ggplot(raw, aes(x = visits, fill = medicaid) ) + geom_histogram(position = &quot;identity&quot;, alpha = 0.5 ) + facet_wrap(~ gender) 4.1.2 密度: geom_density (tidyverse) 男女・メディケイド保有別の利用回数 ggplot(raw, aes(x = visits, fill = medicaid) ) + geom_density(position = &quot;identity&quot;, alpha = 0.5 ) + facet_wrap(~ gender) 4.1.3 Boxplot: geom_boxplot (tidyverse) ggplot(raw, aes(y = visits, x = medicaid) ) + geom_boxplot() 4.2 Y=連続、X=連続 4.2.1 散布図: geom_point (tidyverse) 散布図：連続変数間の関係性を可視化する図 ggplot(raw, aes(x = income, y = visits) ) + geom_point() サンプルサイズが大きくなると機能しない 4.2.2 ヒートマップ: geom_bin2d (tidyverse) 代替案はヒートマップ ggplot(raw, aes(x = income, y = visits) ) + geom_bin2d() "],["予測関数の推定.html", "Chapter 5 予測関数の推定 5.1 データの導入 5.2 データ分割 5.3 OLS 5.4 LASSO/Ridge 5.5 Random Forest/Bagging", " Chapter 5 予測関数の推定 5.1 データの導入 data(&quot;NMES1988&quot;, package = &quot;AER&quot;) raw &lt;- NMES1988 5.2 データ分割 ここでは5個のデータに分割する。 group &lt;- sample(1:5, size = nrow(raw), replace = TRUE) 5.3 OLS 線形予測関数\\(f(X)=\\beta_0 + \\beta_1X_1+...+\\beta_LX_L\\)を仮定し、最小二乗法にて推定する。 fit &lt;- lm(visits ~ ., data = raw[group != 1,]) coef(fit) ## (Intercept) nvisits ovisits novisits emergency ## 3.03433846 0.25114664 -0.01685720 0.15045233 0.18106878 ## hospital healthpoor healthexcellent chronic adllimited ## 1.51178187 1.71621110 -1.40611225 0.78975380 0.70378631 ## regionnortheast regionmidwest regionwest age afamyes ## 0.72783414 -0.07727023 0.73907964 -0.26784051 -0.28379060 ## gendermale marriedyes school income employedyes ## -0.41229255 -0.24555157 0.11966824 -0.01425030 0.65975272 ## insuranceyes medicaidyes ## 1.29210221 1.24792922 予測値の導出 Y.hat &lt;- predict(fit,raw) テストデータへの適合 mean((raw$visits - Y.hat)[group == 1]^2) ## [1] 38.77489 訓練データへの適合 mean((raw$visits - Y.hat)[group != 1]^2) ## [1] 37.57541 5.4 LASSO/Ridge glmentパッケージ(Friedman et al. 2021)を利用 5.5 Random Forest/Bagging rangerパッケージ(Wright, Wager, and Probst 2020)を利用 "],["単一のパラメータの推定.html", "Chapter 6 単一のパラメータの推定 6.1 データ 6.2 部分線形モデルに基づく推定", " Chapter 6 単一のパラメータの推定 （条件付き）平均差を推定する。 点推定だけでなく、信頼区間も推定する。 6.1 データ library(tidyverse) data(&quot;NMES1988&quot;, package = &quot;AER&quot;) raw &lt;- NMES1988 6.2 部分線形モデルに基づく推定 部分線形モデルに関心のあるパラメータを埋め込む \\[E[Y|D=d,X=x]=\\underbrace{\\tau}_{Interest\\ parameter}\\times d+\\underbrace{f(x)}_{Nuisance\\ function}\\] 6.2.1 OLS by lm_robust (estimatr) \\(\\tau(x)=\\tau,f(x)=\\beta_0+\\beta_1x_1+...+\\beta_Lx_L\\)と特定化 サンプル内MSEを最大化するように推定 robust standard errorを計算するためにestimatrパッケージ(Blair et al. 2021)を利用 library(estimatr) lm_robust関数で推定 lm_robust(visits ~ insurance + age + gender + school + income + employed + region + afam + married, data = raw) ## Estimate Std. Error t value Pr(&gt;|t|) CI Lower ## (Intercept) 4.32086605 1.30928537 3.3001713 9.739642e-04 1.75400683 ## insuranceyes 0.96591490 0.24785140 3.8971533 9.877947e-05 0.48000123 ## age 0.04293421 0.15977094 0.2687235 7.881551e-01 -0.27029737 ## gendermale -0.46726544 0.22050124 -2.1191057 3.413748e-02 -0.89955902 ## school 0.08475793 0.03108035 2.7270588 6.415540e-03 0.02382479 ## income -0.04678801 0.03712934 -1.2601357 2.076873e-01 -0.11958023 ## employedyes -0.34186375 0.42407955 -0.8061312 4.202108e-01 -1.17327342 ## regionnortheast 0.34814737 0.30392728 1.1454957 2.520663e-01 -0.24770327 ## regionmidwest -0.40583622 0.25526934 -1.5898354 1.119439e-01 -0.90629278 ## regionwest 0.57163030 0.30418463 1.8792215 6.028038e-02 -0.02472489 ## afamyes -0.39294341 0.34701206 -1.1323624 2.575439e-01 -1.07326195 ## marriedyes -0.29559842 0.23244624 -1.2716851 2.035523e-01 -0.75131020 ## CI Upper DF ## (Intercept) 6.88772528 4394 ## insuranceyes 1.45182856 4394 ## age 0.35616578 4394 ## gendermale -0.03497187 4394 ## school 0.14569108 4394 ## income 0.02600421 4394 ## employedyes 0.48954591 4394 ## regionnortheast 0.94399802 4394 ## regionmidwest 0.09462034 4394 ## regionwest 1.16798549 4394 ## afamyes 0.28737512 4394 ## marriedyes 0.16011336 4394 発展:推計結果表 tidy関数により推定結果data.frameに変化することで、kable関数(knitrパッケージ)による推計結果表の整形、geom_pointrange関数による可視化が可能 点推定値(estimate)、標準誤差(std.error)のみを残した推計結果表 library(knitr) library(tidyverse) fit &lt;- lm_robust(visits ~ insurance + age + gender + school + income + employed + region + afam + married, data = raw) fit &lt;- tidy(fit) fit &lt;- select(fit, term, estimate, std.error) kable(fit, digits = 2) term estimate std.error (Intercept) 4.32 1.31 insuranceyes 0.97 0.25 age 0.04 0.16 gendermale -0.47 0.22 school 0.08 0.03 income -0.05 0.04 employedyes -0.34 0.42 regionnortheast 0.35 0.30 regionmidwest -0.41 0.26 regionwest 0.57 0.30 afamyes -0.39 0.35 marriedyes -0.30 0.23 fit &lt;- filter(fit, term == &quot;insuranceyes&quot;) kable(fit, digits = 2) term estimate std.error insuranceyes 0.97 0.25 発展:Dot-and-Whisker plotによる可視化 Dot-and-Whisker図により点推定量と信頼区間を可視化 fit &lt;- lm_robust(visits ~ insurance + age + gender + school + income + employed + region + afam + married, data = raw) fit &lt;- tidy(fit) fit &lt;- filter(fit, term != &quot;(Intercept)&quot;) ggplot(fit, aes(y = term, x = estimate, xmin = conf.low, xmax = conf.high)) + geom_pointrange() fit &lt;- filter(fit, term == &quot;insuranceyes&quot;) ggplot(fit, aes(y = term, x = estimate, xmin = conf.low, xmax = conf.high)) + geom_pointrange() + geom_vline(xintercept = 0) 6.2.2 ロビンソン変換(Robinson 1988) 部分線形モデルをロビンソン変換(Robinson 1988) \\[Y_i-\\underbrace{E[Y_i|X_i]}_{Nuisance\\ term}=\\tau\\times [D_i-\\underbrace{E[D_i|X_i]}_{Nuisance\\ term}]+u_i\\] \\(E[Y_i|X_i],E[D_i|X_i]\\)を予測関数として推定し、予測誤差間を単回帰すればよい 実際には\\(E[Y_i|X_i],E[D_i|X_i]\\)は未知の関数なので何らかの方法で推定する必要がある。関数の推定なので予測の手法が適用できる。 6.2.2.1 Double selection: rlassoEffect (hdm) 2重選択法(Belloni, Chernozhukov, and Hansen 2014)を紹介 LASSOにより\\(Y_i,D_i\\)の両方あるいはどちらか一方を予測する上でrelevantな\\(X^c\\)を特定しコントロールする \\(Y_i,D_i\\)どちらの予測にもrelevantではない変数は除外する hdmパッケージ(Spindler, Chernozhukov, and Hansen 2019)を利用 library(hdm) Y &lt;- raw$visits D &lt;- if_else(raw$insurance == &quot;yes&quot;, 1, 0) X &lt;- model.matrix(~ - 1+ age + gender + school + income + employed + region + afam + married, raw) fit &lt;- rlassoEffect(x = X, y = Y, d = D, method = &quot;double selection&quot;) 推定結果 summary(fit) ## [1] &quot;Estimates and significance testing of the effect of target variables&quot; ## Estimate. Std. Error t value Pr(&gt;|t|) ## d1 0.9597 0.2480 3.87 0.000109 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 選択されたコントロール変数 fit$selection.index ## age genderfemale gendermale school income ## FALSE FALSE FALSE TRUE TRUE ## employedyes regionnortheast regionmidwest regionwest afamyes ## FALSE FALSE TRUE FALSE TRUE ## marriedyes ## TRUE 6.2.2.2 Double Machine Learning (DoubleML) Double Machine Learning法(Chernozhukov et al. 2018)を紹介 なんらかの方法（例、OLS、ランダムフォレスト、LASSO）で\\(E[Y|X],E[D|X]\\)の予測関数\\(f_Y(X),f_D(X)\\)を推定し、予測誤差を単回帰 DoubleMLパッケージ(Bach et al. 2021)を利用 library(DoubleML) library(mlr3) library(mlr3learners) library(data.table) learner &lt;- lrn(&quot;regr.ranger&quot;, num.trees = 100) # Require bigger num.trees in practice ml_g &lt;- learner$clone() ml_m &lt;- learner$clone() obj_dml_data &lt;- double_ml_data_from_matrix(X = X, y = Y, d = D) dml_plr_obj &lt;- DoubleMLPLR$new(obj_dml_data, ml_g, ml_m, dml_procedure=&quot;dml1&quot;, n_rep = 3) dml_plr_obj$fit() ## INFO [15:45:10.943] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_g&#39; (iter 1/5) ## INFO [15:45:11.777] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_g&#39; (iter 2/5) ## INFO [15:45:11.986] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_g&#39; (iter 4/5) ## INFO [15:45:12.190] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_g&#39; (iter 5/5) ## INFO [15:45:12.391] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_g&#39; (iter 3/5) ## INFO [15:45:12.978] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_m&#39; (iter 2/5) ## INFO [15:45:13.161] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_m&#39; (iter 5/5) ## INFO [15:45:13.326] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_m&#39; (iter 1/5) ## INFO [15:45:13.504] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_m&#39; (iter 3/5) ## INFO [15:45:13.679] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_m&#39; (iter 4/5) ## INFO [15:45:14.110] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_g&#39; (iter 1/5) ## INFO [15:45:14.298] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_g&#39; (iter 3/5) ## INFO [15:45:14.494] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_g&#39; (iter 4/5) ## INFO [15:45:14.694] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_g&#39; (iter 5/5) ## INFO [15:45:14.884] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_g&#39; (iter 2/5) ## INFO [15:45:15.164] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_m&#39; (iter 3/5) ## INFO [15:45:15.510] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_m&#39; (iter 1/5) ## INFO [15:45:15.688] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_m&#39; (iter 2/5) ## INFO [15:45:15.860] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_m&#39; (iter 5/5) ## INFO [15:45:16.030] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_m&#39; (iter 4/5) ## INFO [15:45:16.271] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_g&#39; (iter 2/5) ## INFO [15:45:16.476] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_g&#39; (iter 5/5) ## INFO [15:45:16.681] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_g&#39; (iter 3/5) ## INFO [15:45:16.872] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_g&#39; (iter 4/5) ## INFO [15:45:17.069] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_g&#39; (iter 1/5) ## INFO [15:45:17.328] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_m&#39; (iter 5/5) ## INFO [15:45:17.500] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_m&#39; (iter 4/5) ## INFO [15:45:17.676] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_m&#39; (iter 3/5) ## INFO [15:45:17.853] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_m&#39; (iter 2/5) ## INFO [15:45:18.024] [mlr3] Applying learner &#39;regr.ranger&#39; on task &#39;nuis_m&#39; (iter 1/5) print(dml_plr_obj) ## ================= DoubleMLPLR Object ================== ## ## ## ------------------ Data summary ------------------ ## Outcome variable: y ## Treatment variable(s): d ## Covariates: X1, X2, X3, X4, X5, X6, X7, X8, X9, X10, X11 ## Instrument(s): ## No. Observations: 4406 ## ## ------------------ Score &amp; algorithm ------------------ ## Score function: partialling out ## DML algorithm: dml1 ## ## ------------------ Machine learner ------------------ ## ml_g: regr.ranger ## ml_m: regr.ranger ## ## ------------------ Resampling ------------------ ## No. folds: 5 ## No. repeated sample splits: 3 ## Apply cross-fitting: TRUE ## ## ------------------ Fit summary ------------------ ## [1] &quot;Estimates and significance testing of the effect of target variables&quot; ## Estimate. Std. Error t value Pr(&gt;|t|) ## d 1.2198 0.2569 4.747 2.06e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 "],["references.html", "References", " References Bach, Philipp, Victor Chernozhukov, Malte S. Kurz, and Martin Spindler. 2021. DoubleML: Double Machine Learning in r. https://CRAN.R-project.org/package=DoubleML. Belloni, Alexandre, Victor Chernozhukov, and Christian Hansen. 2014. “Inference on Treatment Effects After Selection Among High-Dimensional Controls.” The Review of Economic Studies 81 (2): 608–50. Blair, Graeme, Jasper Cooper, Alexander Coppock, Macartan Humphreys, and Luke Sonnet. 2021. Estimatr: Fast Estimators for Design-Based Inference. https://CRAN.R-project.org/package=estimatr. Chernozhukov, Victor, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, and James Robins. 2018. “Double/Debiased Machine Learning for Treatment and Structural Parameters: Double/Debiased Machine Learning.” The Econometrics Journal 21 (1). Friedman, Jerome, Trevor Hastie, Rob Tibshirani, Balasubramanian Narasimhan, Kenneth Tay, and Noah Simon. 2021. Glmnet: Lasso and Elastic-Net Regularized Generalized Linear Models. https://CRAN.R-project.org/package=glmnet. James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. An Introduction to Statistical Learning. Vol. 112. Springer. Kleiber, Christian, and Achim Zeileis. 2008. Applied Econometrics with R. New York: Springer-Verlag. https://CRAN.R-project.org/package=AER. Robinson, Peter. 1988. “Root-n-Consistent Semiparametric Regression.” Econometrica 56 (4): 931–54. Spindler, Martin, Victor Chernozhukov, and Christian Hansen. 2019. Hdm: High-Dimensional Metrics. https://CRAN.R-project.org/package=hdm. Wickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain Francois, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686. Wickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen, Kohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani, and Dewey Dunnington. 2020. Ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics. https://CRAN.R-project.org/package=ggplot2. Wickham, Hadley, Romain Francois, Lionel Henry, and Kirill Muller. 2021. Dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr. Wright, Marvin N., Stefan Wager, and Philipp Probst. 2020. Ranger: A Fast Implementation of Random Forests. https://github.com/imbs-hl/ranger. "]]
