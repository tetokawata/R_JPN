
# 予測 {#prediction}

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE)
```

- データと同じ母集団から新しくランダムサンプリングされ、$X$のみが観察できるサンプルについて$Y$の値を予測する。

  - データ分割を用いたモデル評価を行う

- Chapter \@ref(purpose) : 予測問題の論点を紹介

- Chapter \@ref(samplesplit) : データ分割

- Chapter \@ref(OLS) : 線形モデルをOLSで推定する手法を紹介

- Chapter \@ref(LASSO)-\@ref(Ridge) : 線形モデルを罰則付き回帰(LASSO, Ridge)で推定する手法を紹介

- Chapter \@ref(Tree) : 予測木モデルを推定する手法を紹介

- Chapter \@ref(Bagging) : 予測木モデルをモデル平均加法(Bagging, Random Forest)で推定する手法を紹介

## 問題設定 {#purpose}

- 事前に定義する損失関数の期待値を最小化するような、予測関数$f(X)$の推定を目指す。

  - 本ページではMean squared error(MSE)を損失関数として用いるケースを紹介する。所与の$f(X)$、母分布に従う確率変数$Y,X$についてMSEは以下のように定義される。

$$MSE = E_{X,Y}[(Y_i-f(X_i))^2]$$

  - 一般にMSEは以下のように書き換えられる。

$$MSE = \underbrace{E_{X,Y}[(Y_i-\bar{Y}(X_i))^2]}_{Irreducible\ error}+\underbrace{E_{X,Y}[(\bar{Y}(X_i)-f(X_i))^2]}_{Reducible\ error}$$
ただし$\bar{Y}(X_i)=E[Y_i|X_i]$。
上記式から以下が確認できる。

- 最善の予測関数 $\iff$ Reducible error $=0$ $\iff$ 条件付き母平均$\bar{Y}(X_i)$
  
  - 最善の予測関数のもとでも削減不可能なエラー(Irreducible error)が存在
  
  - 予測関数の推定 $=$ Reducible errorの削減 $=$ 条件付き母平均との乖離(MSE)の削減

### Bias-Variance tradeoff

- 実際の$f(X_i)$はランダムサンプリングされたデータから推定される。このためデータの入手前の段階では、確率分布を持つ。

  - Reducible errorは一般に以下のように書き換えられます。

$$E_{Y,X,f(X)}[(\bar{Y}(X_i)-f(X_i))^2]$$

$$=\underbrace{(E_{Y,X,f(X)}[\bar{Y}(X_i)-\bar{f}(X_i)])^2}_{Bias}+\underbrace{E_{Y,X,f(X)}[(\bar{f}(X_i)-f(X_i))^2]}_{Variance}.$$
ただし$\bar{f}(X_i)=f(X_i)$。

- 上記式は推定される予測関数が平均的にどの程度条件付き母平均を近似できているのか(Biasがどの程度小さいのか)だけでなく、予測関数の分布がどの程度散らばっているのか、についても考慮する必要性を示す。

  - 母平均$\bar Y(X_i)$が単純な既知の関数形に従い、かつサンプルサイズが大きい場合、OLS推定された$f(X_i)$は$Bias=0$かつ小さなVarianceを達成する。
  
  - しかしながら社会科学における多くの応用においては、$\bar Y(X_i)$は未知かつ複雑であることが予想され、その複雑さに対してサンプルサイズが小さいことが想定される。

  - このような状況では、深刻なBias-Variance tradeoffに直面する。

  - 少ないパラメータ（短い回帰式、少ないサブサンプル分割）を推定する場合、大きなBiasを持つ
  
  - 多くのパラメータ（長い回帰式、多いサブサンプル分割）を推定する場合、大きなVarianceを持つ。

- Bias-variance tradeoffを分析者が解くことは一般に困難であり、\@ref{LASSO} - \@ref(RandomForest) で紹介するLASSO/Ridge/Random Forestなどの手法は、bias-variance問題をよりデータ主導型かつ現実的な計算時間の手法で解決することを目指す。

## パッケージ & データ

- 利用するパッケージ

```{r}
library(tidyverse)
library(AER)
library(SuperLearner) # 機械学習を実装するメタパッケージ
library(rpart.plot) # 予測木の可視化
```

- データ

  - 元データを結果変数、予測変数データに分割する必要がある

```{r}
data("NMES1988")

Y <- NMES1988$emergency # 結果変数

X <- select(NMES1988,
            -emergency
            ) # 予測変数

set.seed(123)
```

## 事前準備 {#samplesplit}

- ここでは5個のデータに分割する。

```{r}
group <- sample(1:5, # 1から5までの数字を発生される
                size = length(Y), # サンプルサイズと同数発生される
                replace = TRUE) # 同じ数字が発生することを許容する
```

- 第1データをテストデータ、2－5データを訓練データとして使用する

## OLS {#OLS}

- 線形予測関数$f(X)=\beta_0 + \beta_1X_1+...+\beta_LX_L$を想定

  - $\beta_0,...,\beta_L$を最小二乗法にて推定
  
$$\min\sum_i (Y_i-f(X))^2$$

- SuperLeanerによる推定と推定されたモデル

```{r}
fit <-
  SuperLearner(Y = Y[group != 1],
               X = X[group != 1,],
               SL.library = c("SL.lm")
               ) # 推定

coef(fit$fitLibrary$SL.lm_All$object) # 係数値の表示
```

- 予測値の計算

```{r}
Y.pred <- predict(fit, X)$pred
```

- テストデータへの適合

```{r}
mean((Y - Y.pred)[group == 1]^2)/var(Y[group == 1])
```

- 訓練データ

```{r}
mean((Y - Y.pred)[group != 1]^2)/var(Y[group != 1])
```


## LASSO {#LASSO}

- LASSO推定：線形モデルを以下の最適化問題の解として推定

$$\min\sum_i (Y_i-f(X_i))^2+\underbrace{\lambda\sum_l|\beta_l|}_{Penalty\ term}$$

  - $\lambda$ : チューニングパラメタ、Cross-validationを用いて設定可能

- glmentパッケージ[@R-glmnet]を利用

  - glmnetはdata.frameを直接の入力できず、matrix(vector)に変換する必要がある


```{r}
fit <-
  SuperLearner(Y = Y[group != 1],
               X = X[group != 1,],
               SL.library = c("SL.glmnet")
               )

coef(fit$fitLibrary$SL.glmnet_All$object)
```


- テストデータへの適合

```{r}
mean((Y - Y.pred)[group == 1]^2)/var(Y[group == 1])
```

- 訓練データ

```{r}
mean((Y - Y.pred)[group != 1]^2)/var(Y[group != 1])
```


## Ridge {#Ridge}

- Ridge推定：線形モデルを以下の最適化問題の解として推定

$$\min\sum_i (Y_i-f(X_i))^2+\underbrace{\lambda\sum_l(\beta_l)^2}_{Penalty\ term}$$

  - $\lambda$ : チューニングパラメタ、Cross-validationを用いて設定可能

- 引き続きglmentパッケージ[@R-glmnet]を利用

```{r}
learners = create.Learner("SL.glmnet", params = list(alpha = 0)) # glmnetのalphaを0（Ridge推定）に設定

fit <-
  SuperLearner(Y = Y,
               X = X,
               SL.library = c(learners$names)
               )

coef(fit$fitLibrary$SL.glmnet_1_All$object)
```


- テストデータへの適合

```{r}
mean((Y - Y.pred)[group == 1]^2)/var(Y[group == 1])
```

- 訓練データ

```{r}
mean((Y - Y.pred)[group != 1]^2)/var(Y[group != 1])
```

## Tree {#Tree}

- 予測木をrpartパッケージにより推定する

- 推定法は以下の通り

1. ある変数$X$のある閾値$\bar X$において、サンプルを分割する

  - 分割後のサブサンプル平均を暫定的予測値とする
  
  - 予測値と訓練データにおける結果変数の値の乖離(MSE)を最小にするように、分割に用いる変数と閾値を決定

2. １回目の分割結果を所与として、２回目の分割を決定。変数と閾値は１回目と同様の基準で決定。

3. 以上を繰り返す

4. 推定された予測木について、pruningを実行

```{r}
fit <-
  SuperLearner(Y = Y,
               X = X,
               SL.library = c("SL.rpartPrune")
               )

rpart.plot(fit$fitLibrary$SL.rpartPrune_All$object) # 予測木の可視化
```


- テストデータへの適合

```{r}
mean((Y - Y.pred)[group == 1]^2)/var(Y[group == 1])
```

- 訓練データ

```{r}
mean((Y - Y.pred)[group != 1]^2)/var(Y[group != 1])
```


## Random Forest {#Bagging}

- rangerパッケージ[@R-ranger]を利用

```{r}
fit <-
  SuperLearner(Y = Y,
               X = X,
               SL.library = c("SL.ranger")
               )
```


- テストデータへの適合

```{r}
mean((Y - Y.pred)[group == 1]^2)/var(Y[group == 1])
```

- 訓練データ

```{r}
mean((Y - Y.pred)[group != 1]^2)/var(Y[group != 1])
```

## Stacking

- 交差検証法を用いて、複数の推定アルゴリズムの性能を比較する

  - 複数の予測モデルを線型結合した予測モデル（Super learner: @van2007super）も推定し、性能評価
  
```{r}
learners = create.Learner("SL.glmnet", params = list(alpha = 0)) # Ridge推定の定義

fit <-
  CV.SuperLearner(X = X,
                  Y = Y,
                  SL.library = c("SL.mean",
                                 "SL.lm",
                                 "SL.glmnet",
                                 "SL.rpartPrune",
                                 "SL.ranger",
                                 learners$names
                                 )
                  ) # モデル推定 & 交差検証

summary(fit)
```

