
# 予測 {#prediction}

## 問題設定

- データと同じ母集団から新しくランダムサンプリングされ、$X$のみが観察できるサンプルについて$Y$の値を予測することを目的とします。

- 具体的には、事前に定義する損失関数の期待値を最小化するような、予測関数$f(X)$の推定をめざします。
以下ではMean squared error(MSE)を損失関数として用います。
所与の$f(X)$、母分布に従う確率変数$Y,X$についてMSEは以下のように定義されます。

$$MSE = E_{X,Y}[(Y_i-f(X_i))^2]$$

- 一般にMSEは以下のように書き換えられます。

$$MSE = \underbrace{E_{X,Y}[(Y_i-\bar{Y}(X_i))^2]}_{Irreducible\ error}+\underbrace{E_{X,Y}[(\bar{Y}(X_i)-f(X_i))^2]}_{Reducible\ error}$$
ただし$\bar{Y}(X_i)=E[Y_i|X_i]$。
上記式から以下が確認できます。

  - 最善の予測関数は条件付き母平均$\bar{Y}(X_i)$(Reducible error = 0)
  
  - 最善の予測関数のもとでも削減不可能なエラー(Irreducible error)が存在
  
  - 予測関数の推定 $=$ Reducible errorの削減 $=$ 条件付き母平均との乖離(MSE)の削減

### Bias-Variance tradeoff

- 実際の$f(X_i)$はランダムサンプリングされたデータから推定される必要があり、実際には確率分布を持ちます。

- Reducible errorは一般に以下のように書き換えられます。

$$E_{Y,X,f(X)}[(\bar{Y}(X_i)-f(X_i))^2]$$

$$=\underbrace{(E_{Y,X,f(X)}[\bar{Y}(X_i)-\bar{f}(X_i)])^2}_{Bias}+\underbrace{E_{Y,X,f(X)}[(\bar{f}(X_i)-f(X_i))^2]}_{Variance}.$$
ただし$\bar{f}(X_i)=f(X_i)$。

- 上記式は推定される予測関数が平均的にどの程度条件付き母平均を近似できているのか(Biasがどの程度小さいのか)だけでなく、予測関数の分布がどの程度散らばっているのか（VArianceがどの程度大木のか）、についても考慮する必要があることを示しています。

- 母平均$\bar Y(X_i)$が単純な既知の関数形に従い、かつサンプルサイズが大きい場合、OLS推定された$f(X_i)$は$Bias=0$かつ小さいVarianceを達成します。
しかしながら社会科学における多くの応用においては、$\bar Y(X_i)$は未知かつ複雑であることが予想され、その複雑さに対してサンプルサイズが小さいことを想定する必要があります。
このような状況では、OLSやサブサンプル平均により推定された予測モデルは、Bias-Variance tradeoffに直面します。

  - 少ないパラメータ（短い回帰式、少ないサブサンプル分割）を推定する場合、大きなBiasを持つ
  
  - 多くのパラメータ（長い回帰式、多いサブサンプル分割）を推定する場合、大きなVarianceを持つ。

- Bias-variance tradeoffを分析者が解くこと（最善のモデル設定を行うこと）は困難です。Chapter \@ref(prediction) で紹介するLASSO/Ridge/Random Forestなどの手法は、bias-varianceのバランスをよりデータ主導型かつ現実的な計算時間で達成することを目指します。

## データの導入

```{r}
data("NMES1988",
     package = "AER")

raw <- NMES1988
```

## データ分割

- ここでは5個のデータに分割する。

```{r}
group <- sample(1:5,
                size = nrow(raw),
                replace = TRUE)
```


## OLS

- 線形予測関数$f(X)=\beta_0 + \beta_1X_1+...+\beta_LX_L$を仮定し、最小二乗法にて推定する。

```{r}
fit <- lm(visits ~ .,
          data = raw[group != 1,])

coef(fit)
```

- 予測値の導出

```{r}
Y.hat <- predict(fit,raw)
```

- テストデータへの適合

```{r}
mean((raw$visits - Y.hat)[group == 1]^2)
```

- 訓練データへの適合

```{r}
mean((raw$visits - Y.hat)[group != 1]^2)
```

## LASSO

- glmentパッケージ[@R-glmnet]を利用

```{r}
library(glmnet)

Y <- raw$visits

X <- model.matrix(visits ~ -1 + .,
                  data = raw)

cv <- cv.glmnet(x = X[group != 1,],
                y = Y[group != 1],
                alpha = 1)

fit <- glmnet(x = X[group != 1,],
              y = Y[group != 1],
              alpha = 1,
              lambda = cv$lambda.min)
```

- 予測モデルの確認

```{r}
coef(fit)
```


- 予測値の導出

```{r}
Y.hat <- predict(fit,X)
```

- テストデータへの適合

```{r}
mean((Y - Y.hat)[group == 1]^2)
```

- 訓練データへの適合

```{r}
mean((Y - Y.hat)[group != 1]^2)
```


## Rdige

- 引き続きglmentパッケージ[@R-glmnet]を利用

```{r}
cv <- cv.glmnet(x = X[group != 1,],
                y = Y[group != 1],
                alpha = 0)

fit <- glmnet(x = X[group != 1,],
              y = Y[group != 1],
              alpha = 0,
              lambda = cv$lambda.min)
```

- 予測モデルの確認

```{r}
coef(fit)
```


- 予測値の導出

```{r}
Y.hat <- predict(fit,X)
```

- テストデータへの適合

```{r}
mean((Y - Y.hat)[group == 1]^2)
```

- 訓練データへの適合

```{r}
mean((Y - Y.hat)[group != 1]^2)
```

## Bagging

- rangerパッケージ[@R-ranger]を利用

```{r}
library(ranger)

fit <- ranger(x = X[group != 1,],
              y = Y[group != 1],
              num.trees = 2000,
              mtry = ncol(X))
```

- 予測値の計算

```{r}
Y.hat <- predict(fit,X)$predictions
```


- テストデータへの適合

```{r}
mean((Y - Y.hat)[group == 1]^2)
```

- 訓練データへの適合

```{r}
mean((Y - Y.hat)[group != 1]^2)
```

## Random Forest

- rangerパッケージ[@R-ranger]を利用

```{r}
library(ranger)

fit <- ranger(x = X[group != 1,],
              y = Y[group != 1],
              num.trees = 2000)
```

- 予測値の計算

```{r}
Y.hat <- predict(fit,X)$predictions
```


- テストデータへの適合

```{r}
mean((Y - Y.hat)[group == 1]^2)
```

- 訓練データへの適合

```{r}
mean((Y - Y.hat)[group != 1]^2)
```
