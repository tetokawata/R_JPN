
# 予測 {#prediction}

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE)
```

- データと同じ母集団から新しくランダムサンプリングされ、$X$のみが観察できるサンプルについて$Y$の値を予測する。

  - データ分割を用いたモデル評価を行う

- Chapter \@ref(purpose) : 予測問題の論点を紹介

- Chapter \@ref(samplesplit) : データ分割

- Chapter \@ref(OLS) : 線形モデルをOLSで推定する手法を紹介

- Chapter \@ref(LASSO)-\@ref(Ridge) : 線形モデルを罰則付き回帰(LASSO, Ridge)で推定する手法を紹介

- Chapter \@ref(Tree) : 予測木モデルを推定する手法を紹介

- Chapter \@ref(Bagging) : 予測木モデルをモデル平均加法(Bagging, Random Forest)で推定する手法を紹介

## 問題設定 {#purpose}

- 事前に定義する損失関数の期待値を最小化するような、予測関数$f(X)$の推定を目指す。

  - 本ページではMean squared error(MSE)を損失関数として用いるケースを紹介する。所与の$f(X)$、母分布に従う確率変数$Y,X$についてMSEは以下のように定義される。

$$MSE = E_{X,Y}[(Y_i-f(X_i))^2]$$

  - 一般にMSEは以下のように書き換えられる。

$$MSE = \underbrace{E_{X,Y}[(Y_i-\bar{Y}(X_i))^2]}_{Irreducible\ error}+\underbrace{E_{X,Y}[(\bar{Y}(X_i)-f(X_i))^2]}_{Reducible\ error}$$
ただし$\bar{Y}(X_i)=E[Y_i|X_i]$。
上記式から以下が確認できる。

- 最善の予測関数 $\iff$ Reducible error $=0$ $\iff$ 条件付き母平均$\bar{Y}(X_i)$
  
  - 最善の予測関数のもとでも削減不可能なエラー(Irreducible error)が存在
  
  - 予測関数の推定 $=$ Reducible errorの削減 $=$ 条件付き母平均との乖離(MSE)の削減

### Bias-Variance tradeoff

- 実際の$f(X_i)$はランダムサンプリングされたデータから推定される。このためデータの入手前の段階では、確率分布を持つ。

  - Reducible errorは一般に以下のように書き換えられます。

$$E_{Y,X,f(X)}[(\bar{Y}(X_i)-f(X_i))^2]$$

$$=\underbrace{(E_{Y,X,f(X)}[\bar{Y}(X_i)-\bar{f}(X_i)])^2}_{Bias}+\underbrace{E_{Y,X,f(X)}[(\bar{f}(X_i)-f(X_i))^2]}_{Variance}.$$
ただし$\bar{f}(X_i)=f(X_i)$。

- 上記式は推定される予測関数が平均的にどの程度条件付き母平均を近似できているのか(Biasがどの程度小さいのか)だけでなく、予測関数の分布がどの程度散らばっているのか、についても考慮する必要性を示す。

  - 母平均$\bar Y(X_i)$が単純な既知の関数形に従い、かつサンプルサイズが大きい場合、OLS推定された$f(X_i)$は$Bias=0$かつ小さなVarianceを達成する。
  
  - しかしながら社会科学における多くの応用においては、$\bar Y(X_i)$は未知かつ複雑であることが予想され、その複雑さに対してサンプルサイズが小さいことが想定される。

  - このような状況では、深刻なBias-Variance tradeoffに直面する。

  - 少ないパラメータ（短い回帰式、少ないサブサンプル分割）を推定する場合、大きなBiasを持つ
  
  - 多くのパラメータ（長い回帰式、多いサブサンプル分割）を推定する場合、大きなVarianceを持つ。

- Bias-variance tradeoffを分析者が解くことは一般に困難であり、\@ref{LASSO} - \@ref(RandomForest) で紹介するLASSO/Ridge/Random Forestなどの手法は、bias-variance問題をよりデータ主導型かつ現実的な計算時間の手法で解決することを目指す。

## パッケージ & データ
- Rによる教師付き学習実装法には、大きく二つの有力な選択肢が存在する

  - 個別の推定法を実装するパッケージ (例：LASSO & Ridge $=$ glmnet, Random Forest $=$ ranger) を使用
  
    - dependencyを減らすことができ、パッケージの更新にも迅速に対応できる
  
  - 個別パッケージで実装される手法を”共通”の文法で使用するメタパッケージ(例： caret, mlr3, tidymodels, **SuerLeaner**)を使用
  
    - 初学者が色々な手法を試すことが容易
  
- 本ページでは、SuperLearnerの利用を前提にしている

  - 他のメタパッケージに比べて”自由度”が低いが、コーディングに慣れていない読者にとっても比較的用意に使える（かも。。。）
  
  - 後述するStacking法の実装を主目的としており、母集団への先見的知見が少ない社会科学において実用的
  
- 利用するパッケージ

```{r}
library(tidyverse)
library(AER)
library(SuperLearner) # 機械学習を実装するメタパッケージ
library(rpart.plot) # 予測木の可視化
```



- データ

  - 元データを結果変数、予測変数データに分割する必要がある

```{r}
data("NMES1988")

Y <- NMES1988$emergency # 結果変数

X <- select(NMES1988,
            -emergency
            ) # 予測変数

set.seed(123)
```

## 事前準備 {#samplesplit}

- ここでは5個のデータに分割する。

```{r}
group <- sample(1:5, # 1から5までの数字を発生される
                size = length(Y), # サンプルサイズと同数発生される
                replace = TRUE) # 同じ数字が発生することを許容する
```

- 第1データをテストデータ、2－5データを訓練データとして使用する

## OLS {#OLS}

- 線形予測関数$f(X)=\beta_0 + \beta_1X_1+...+\beta_LX_L$を想定

  - $\beta_0,...,\beta_L$を最小二乗法にて推定
  
$$\min\sum_i (Y_i-f(X))^2$$

- SuperLeanerによる推定と推定されたモデル

```{r}
fit <-
  SuperLearner(Y = Y[group != 1],
               X = X[group != 1,],
               SL.library = c("SL.lm"),
               cvControl = list(V = 20L)
               ) # 推定

coef(fit$fitLibrary$SL.lm_All$object) # 係数値の表示
```

- 元データ全体への予測値の計算

```{r}
Y.pred <- predict(fit,X)$pred
```


- テストデータへの適合

```{r}
mean((Y - Y.pred)[group == 1]^2)
```

- 訓練データへの適合

```{r}
mean((Y - Y.pred)[group != 1]^2)
```

- デフォルトの設定では、訓練データのみを用いた交差検証（10分割）の結果も自動的に計算されている

```{r}
fit
```

- Risk $=$ 交差検証法で推定されたMSE

- Coef $=$ 後述するStackingモデルにおけるOLSによる予測値へのweight

## LASSO {#LASSO}

- LASSO推定：線形モデルを以下の最適化問題の解として推定

$$\min\sum_i (Y_i-f(X_i))^2+\underbrace{\lambda\sum_l|\beta_l|}_{Penalty\ term}$$

  - $\lambda$ : チューニングパラメタ、Cross-validationを用いて設定可能

- glmentパッケージ[@R-glmnet]を利用

  - glmnetはdata.frameを直接の入力できず、matrix(vector)に変換する必要がある


```{r}
fit <-
  SuperLearner(Y = Y[group != 1],
               X = X[group != 1,],
               SL.library = c("SL.glmnet")
               )

coef(fit$fitLibrary$SL.glmnet_All$object)
```

- 元データ全体への予測値の計算

```{r}
Y.pred <- predict(fit,X)$pred
```


- テストデータへの適合

```{r}
mean((Y - Y.pred)[group == 1]^2)
```

- 訓練データへの適合

```{r}
mean((Y - Y.pred)[group != 1]^2)
```


- 交差検証

```{r}
fit
```


## Ridge {#Ridge}

- Ridge推定：線形モデルを以下の最適化問題の解として推定

$$\min\sum_i (Y_i-f(X_i))^2+\underbrace{\lambda\sum_l(\beta_l)^2}_{Penalty\ term}$$

  - $\lambda$ : チューニングパラメタ、Cross-validationを用いて設定可能

- 引き続きglmentパッケージ[@R-glmnet]を利用

```{r}
learners = create.Learner("SL.glmnet", params = list(alpha = 0)) # glmnetのalphaを0（Ridge推定）に設定

fit <-
  SuperLearner(Y = Y,
               X = X,
               SL.library = c(learners$names)
               )

coef(fit$fitLibrary$SL.glmnet_1_All$object)
```


- 元データ全体への予測値の計算

```{r}
Y.pred <- predict(fit,X)$pred
```


- テストデータへの適合

```{r}
mean((Y - Y.pred)[group == 1]^2)
```

- 訓練データへの適合

```{r}
mean((Y - Y.pred)[group != 1]^2)
```


- 交差検証

```{r}
fit
```


## Tree {#Tree}

- 予測木を推定する

- 推定法は以下の通り

1. ある変数$X$のある閾値$\bar X$において、サンプルを分割する

  - 分割後のサブサンプル平均を暫定的予測値とする
  
  - 予測値と訓練データにおける結果変数の値の乖離(MSE)を最小にするように、分割に用いる変数と閾値を決定

2. １回目の分割結果を所与として、２回目の分割を決定。変数と閾値は１回目と同様の基準で決定。

3. 以上を繰り返す

4. 推定された予測木について、pruningを実行

```{r}
fit <-
  SuperLearner(Y = Y,
               X = X,
               SL.library = c("SL.rpartPrune")
               )

rpart.plot(fit$fitLibrary$SL.rpartPrune_All$object) # 予測木の可視化
```


- 元データ全体への予測値の計算

```{r}
Y.pred <- predict(fit,X)$pred
```

- テストデータへの適合

```{r}
mean((Y - Y.pred)[group == 1]^2)
```

- 訓練データへの適合

```{r}
mean((Y - Y.pred)[group != 1]^2)
```


- 交差検証

```{r}
fit
```

## Random Forest {#Bagging}

- Random Forestを推定する

  - 多数の予測木を推定し、各予測値の平均値を最終予測値とする
  
    - 平均を取ることで、予測値の分散削減が期待できる
    
  - 予測木におけるサンプル分割において、ランダムに予測変数の部分集合を選ぶ
  
    - 部分集合の中から、訓練データへの適合度が最大になるように分割を行う
    
    - 各予測木の予測値の相関を減らし、平均化による分散削減を促進する

```{r}
fit <-
  SuperLearner(Y = Y,
               X = X,
               SL.library = c("SL.ranger")
               )
```


- 元データ全体への予測値の計算

```{r}
Y.pred <- predict(fit,X)$pred
```


- テストデータへの適合

```{r}
mean((Y - Y.pred)[group == 1]^2)
```

- 訓練データへの適合

```{r}
mean((Y - Y.pred)[group != 1]^2)
```


- 交差検証

```{r}
fit
```

## Stacking

- SuperLearner関数の主目的は、複数の予測モデルを線型結合した予測モデル（Super learner: @van2007super）の推定

  - $f_k(x)$をアルゴリズム$k$ (例：LASSO, Ridge, Random Forest)により推定された予測値とすると、SuperLearnerは以下のように定義される
  
$$f_{SL}(x)=\beta_1 f_1(x)+...+\beta_K f_{K}(x)$$

ただし, $\beta_k \in [0,1]$かつ$\beta_1+...+\beta_K=1$

```{r}
learners = create.Learner("SL.glmnet", params = list(alpha = 0)) # Ridge推定の定義

fit <-
  SuperLearner(X = X[group != 1,],
               Y = Y[group != 1],
               SL.library = c("SL.mean",
                              "SL.lm",
                              "SL.glmnet",
                              "SL.rpartPrune",
                              "SL.ranger",
                              learners$names
                              )
               ) # モデル推定 & 交差検証

fit
```

- Coef $= \beta_k$

  - 訓練データ内の交差検証では、glmnetにより実装されたLASSOによる予測値の性能が最もいいことが確認できる

  - Stackingモデルにおいて、最もweightが大きいのはLASSO、続いてrangerであることが確認できる
  
- Stackingモデルの予測値  

```{r}
Y.pred <- predict(fit,X)$pred
```


- テストデータへの適合

```{r}
mean((Y - Y.pred)[group == 1]^2)
```

- 訓練データへの適合

```{r}
mean((Y - Y.pred)[group != 1]^2)
```
